{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received event:\n",
      "{\n",
      "    \"event_name\": \"authorizationCreated\",\n",
      "    \"event_order\": 1\n",
      "}\n",
      "Received event:\n",
      "{\n",
      "    \"event_name\": \"authorizationCreated\",\n",
      "    \"event_order\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#flink-job | confluent_kafka.py\n",
    "#First Approach\n",
    "#Consume Kafka Events From Topic\n",
    "#using Confluent Kafka\n",
    "\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "import json\n",
    "\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  \n",
    "    'group.id': 'flink-consumer-group-confluent',        \n",
    "    'auto.offset.reset': 'earliest'         \n",
    "}\n",
    "\n",
    "consumer = Consumer(conf)\n",
    "\n",
    "consumer.subscribe(['dataeng-poc-flink-watermarking'])\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                continue\n",
    "            else:\n",
    "                print(msg.error())\n",
    "                break\n",
    "        else:\n",
    "            try:\n",
    "                event = json.loads(msg.value().decode('utf-8'))\n",
    "                print(\"Received event:\")\n",
    "                print(json.dumps(event, indent=4)) \n",
    "            except Exception as e:\n",
    "                print(\"Error processing message:\", e)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "finally:\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Received:\n",
      "{\n",
      "    \"system_id\": \"IA123456789\",\n",
      "    \"source_system\": \"AuthorisationSwitch\",\n",
      "    \"acceptance_source_system\": \"GatewayAcceptance\",\n",
      "    \"type\": \"PreAuth\",\n",
      "    \"entry_type\": \"Debit\",\n",
      "    \"payment_reference_id\": \"PR123456789\",\n",
      "    \"original_payment_reference_id\": \"OPR123456789\",\n",
      "    \"merchant_payment_reference\": \"MPR123456789\",\n",
      "    \"acquirer_name\": \"AcquirerX\",\n",
      "    \"processor_name\": \"ProcessorY\",\n",
      "    \"stan_number\": \"STAN123\",\n",
      "    \"transaction_creation_date\": {\n",
      "        \"local\": \"2024-02-19T08:00:00\",\n",
      "        \"local_timezone\": \"UTC+0\",\n",
      "        \"utc\": \"2024-02-19T08:00:00\"\n",
      "    },\n",
      "    \"requester\": {\n",
      "        \"origin_ip\": \"192.168.1.1\",\n",
      "        \"origin_platform\": \"Portal\",\n",
      "        \"origin_country_code\": \"US\",\n",
      "        \"username\": \"user123\",\n",
      "        \"fingerprint\": \"ABCDEF123456\"\n",
      "    },\n",
      "    \"merchant\": {\n",
      "        \"id\": \"M123\",\n",
      "        \"system_id\": \"G123\",\n",
      "        \"external_id\": \"EM123\",\n",
      "        \"scheme_merchant_id\": \"SMID123\",\n",
      "        \"acquirer_id\": \"ACQ123\",\n",
      "        \"terminal_id\": \"TID123\",\n",
      "        \"market_category_code\": \"MCC123\"\n",
      "    },\n",
      "    \"market_data\": {},\n",
      "    \"channel\": {\n",
      "        \"type\": \"POS\",\n",
      "        \"payment_site_id\": \"PSID123\",\n",
      "        \"terminal_id\": \"TID123\",\n",
      "        \"info\": {\n",
      "            \"pos_entry_mode\": \"Chip&Pin\",\n",
      "            \"pos_capability\": \"Contactless\",\n",
      "            \"pos_environment\": \"Merchant\",\n",
      "            \"pos_condition_code\": \"00\",\n",
      "            \"pos_condition_description\": \"Normal\",\n",
      "            \"pos_processing_code\": \"000000\",\n",
      "            \"card_holder_verification_method\": \"Signature\"\n",
      "        }\n",
      "    },\n",
      "    \"currency_conversion\": {\n",
      "        \"type\": \"Dynamic\",\n",
      "        \"is_eligible\": true,\n",
      "        \"is_accepted\": true,\n",
      "        \"exchange_rate\": 1.25,\n",
      "        \"markup\": 0.05\n",
      "    },\n",
      "    \"amounts\": {\n",
      "        \"requested\": {\n",
      "            \"currency_code\": \"USD\",\n",
      "            \"currency_exponent\": 2,\n",
      "            \"value\": 100.0\n",
      "        },\n",
      "        \"authorized\": {\n",
      "            \"currency_code\": \"USD\",\n",
      "            \"currency_exponent\": 2,\n",
      "            \"value\": 125.0\n",
      "        },\n",
      "        \"tip\": {\n",
      "            \"currency_code\": \"USD\",\n",
      "            \"currency_exponent\": 2,\n",
      "            \"value\": 5.0\n",
      "        },\n",
      "        \"donation\": {\n",
      "            \"currency_code\": \"USD\",\n",
      "            \"currency_exponent\": 2,\n",
      "            \"value\": 10.0\n",
      "        },\n",
      "        \"vat\": {\n",
      "            \"currency_code\": \"USD\",\n",
      "            \"currency_exponent\": 2,\n",
      "            \"value\": 15.0\n",
      "        }\n",
      "    },\n",
      "    \"payment_method\": {\n",
      "        \"type\": \"Card\",\n",
      "        \"card_type\": \"Credit\",\n",
      "        \"scheme_id\": \"MC\",\n",
      "        \"scheme_name\": \"MASTERCARD INTERNATIONAL\",\n",
      "        \"currency_code\": \"USD\",\n",
      "        \"masked_number\": \"************1234\",\n",
      "        \"bin_number\": \"123456\",\n",
      "        \"last4_number\": \"1234\",\n",
      "        \"issuer\": {\n",
      "            \"name\": \"IssuerX\",\n",
      "            \"country_code\": \"US\"\n",
      "        },\n",
      "        \"token\": {\n",
      "            \"value\": \"TOKEN123456789\",\n",
      "            \"provider\": \"ProviderZ\"\n",
      "        },\n",
      "        \"read_method\": \"Chip&Pin\",\n",
      "        \"cardholder_verification_method\": \"PIN\",\n",
      "        \"three_ds\": {\n",
      "            \"used\": true,\n",
      "            \"is_frictionless\": true,\n",
      "            \"acs_url\": \"https://acsurl.com\",\n",
      "            \"authentication_type\": \"AuthenticationTypeX\",\n",
      "            \"transaction_id\": \"TXID123\",\n",
      "            \"message_version\": \"1.0\",\n",
      "            \"authentication_response\": \"Success\",\n",
      "            \"result_reason\": \"Successful\",\n",
      "            \"exemption_type\": \"LowValue\"\n",
      "        },\n",
      "        \"sca\": {\n",
      "            \"transaction_reference\": \"TXREF123\",\n",
      "            \"transaction_initiator\": \"Merchant\",\n",
      "            \"card_on_file_indicator\": \"NotOnFile\",\n",
      "            \"exemption_indicator\": \"Exempted\",\n",
      "            \"mit_type\": \"TwoFactor\"\n",
      "        },\n",
      "        \"emv\": {\n",
      "            \"application_id\": \"APPID123\",\n",
      "            \"cryptogram\": \"CRYPTO123\",\n",
      "            \"cryptogram_type\": \"TypeA\",\n",
      "            \"unpredictable_number\": \"UN123\",\n",
      "            \"transaction_status_info\": \"StatusInfo123\"\n",
      "        },\n",
      "        \"avs\": {\n",
      "            \"response_code\": \"Match\",\n",
      "            \"partner_response_code\": \"Match\"\n",
      "        },\n",
      "        \"cvv\": {\n",
      "            \"response_code\": \"Match\",\n",
      "            \"partner_response_code\": \"Match\"\n",
      "        }\n",
      "    },\n",
      "    \"metadata\": {\n",
      "        \"merchant\": {},\n",
      "        \"user\": {}\n",
      "    },\n",
      "    \"scheme_response\": {\n",
      "        \"reference_id\": \"REF123\",\n",
      "        \"approval_code\": \"APPROVAL123\",\n",
      "        \"response_code\": \"00\",\n",
      "        \"response_text\": \"Approved\",\n",
      "        \"response_date\": {\n",
      "            \"local\": \"2024-02-19T08:05:00\",\n",
      "            \"local_timezone\": \"UTC+0\",\n",
      "            \"utc\": \"2024-02-19T08:05:00\"\n",
      "        }\n",
      "    },\n",
      "    \"acquirer_response\": {\n",
      "        \"retrieval_reference_number\": \"RRN123\",\n",
      "        \"action_code\": \"\",\n",
      "        \"approval_code\": \"APPROVAL123\",\n",
      "        \"response_code\": \"00\",\n",
      "        \"response_text\": \"Approved\",\n",
      "        \"response_date\": {\n",
      "            \"local\": \"2024-02-19T08:10:00\",\n",
      "            \"local_timezone\": \"UTC+0\",\n",
      "            \"utc\": \"2024-02-19T08:10:00\"\n",
      "        }\n",
      "    },\n",
      "    \"gateway_response\": {\n",
      "        \"approval_code\": \"APPROVAL123\",\n",
      "        \"response_code\": \"00\",\n",
      "        \"response_text\": \"Approved\",\n",
      "        \"response_date\": {\n",
      "            \"local\": \"2024-02-19T08:15:00\",\n",
      "            \"local_timezone\": \"UTC+0\",\n",
      "            \"utc\": \"2024-02-19T08:15:00\"\n",
      "        }\n",
      "    },\n",
      "    \"receipt\": {\n",
      "        \"link\": \"https://example.com/receipt123.pdf\"\n",
      "    },\n",
      "    \"event\": {}\n",
      "}\n",
      "Event Received:\n",
      "{\n",
      "    \"event_name\": \"authorizationCreated\",\n",
      "    \"event_order\": 1\n",
      "}\n",
      "Event Received:\n",
      "{\n",
      "    \"event_name\": \"authorizationCreated\",\n",
      "    \"event_order\": 1\n",
      "}\n",
      "Event Received:\n",
      "{\n",
      "    \"transaction_creation_date\": \"2024-02-19T08:00:00\",\n",
      "    \"acquirer_name\": \"PayPal\",\n",
      "    \"value\": 110.1\n",
      "}\n",
      "Event Received:\n",
      "{\n",
      "    \"transaction_creation_date\": \"2024-02-19T08:00:00\",\n",
      "    \"acquirer_name\": \"PayPal\",\n",
      "    \"value\": 110.1\n",
      "}\n",
      "Event Received:\n",
      "{\n",
      "    \"transaction_creation_date\": \"2024-02-19T08:00:00\",\n",
      "    \"acquirer_name\": \"PayPal\",\n",
      "    \"value\": 110.1\n",
      "}\n",
      "Stopping Consumer\n"
     ]
    }
   ],
   "source": [
    "#flink-job | consume-events-pyflink.py\n",
    "#Second Approach\n",
    "#Consume Kafka Events From Topic \n",
    "#using pyflink\n",
    "\n",
    "from pyflink.common.serialization import SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer\n",
    "from pyflink.table.table\n",
    "import json\n",
    "\n",
    "def main(topic, group_id):\n",
    "    try:\n",
    "        env = StreamExecutionEnvironment.get_execution_environment()\n",
    "        env.set_parallelism(1)\n",
    "        kafka_properties = {\n",
    "            'bootstrap.servers': 'localhost:9092,localhost:9093,local:9094',\n",
    "            'group.id': group_id\n",
    "        }\n",
    "\n",
    "        kafka_consumer = FlinkKafkaConsumer(\n",
    "            topic,\n",
    "            SimpleStringSchema(),  \n",
    "            kafka_properties)  \n",
    "        \n",
    "        kafka_consumer.set_start_from_earliest()\n",
    "        stream = env.add_source(kafka_consumer)\n",
    "        \n",
    "        def process_event(event):\n",
    "            try:\n",
    "                event_json = json.loads(event)\n",
    "                print('Event Received:\\n' + json.dumps(event_json, indent=4))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print('Error decoding JSON:', e)\n",
    "\n",
    "        stream.map(process_event)\n",
    "        #stream.map(lambda x: print('Event Received:\\n'+json.dumps(json.loads(x), indent=4)))\n",
    "        \n",
    "        env.execute(\"Consume Kafka Events\")\n",
    "    except Exception as e:\n",
    "        print('An error occurred:', e)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        print('Stopping Consumer')\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    topic_name = 'dataeng-poc-flink-watermarking'\n",
    "    group_id = 'flink-consumer-group-pyflink'   \n",
    "    main(topic_name, group_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 295, in _execute\n",
      "    response = task()\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 370, in <lambda>\n",
      "    lambda: self.create_worker().do_instruction(request), request)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 629, in do_instruction\n",
      "    return getattr(self, request_type)(\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 667, in process_bundle\n",
      "    bundle_processor.process_bundle(instruction_id))\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 1061, in process_bundle\n",
      "    input_op_by_transform_id[element.transform_id].process_encoded(\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 231, in process_encoded\n",
      "    self.output(decoded_value)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py\", line 528, in output\n",
      "    _cast_to_receiver(self.receivers[output_index]).receive(windowed_value)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py\", line 240, in receive\n",
      "    self.consumer.process(windowed_value)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/beam/beam_operations_slow.py\", line 151, in process\n",
      "    self._main_output_processor.process_outputs(\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/beam/beam_operations_slow.py\", line 65, in process_outputs\n",
      "    self._consumer.process(windowed_value.with_value(results))\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/beam/beam_operations_slow.py\", line 150, in process\n",
      "    for value in o.value:\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/datastream/process/operations.py\", line 197, in wrapped_func\n",
      "    yield from _emit_results(timestamp, watermark, results, has_side_output)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/datastream/process/input_handler.py\", line 131, in _emit_results\n",
      "    for result in results:\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/datastream/data_stream.py\", line 686, in process_element\n",
      "    yield value, self._extract_timestamp_func(value, ctx.timestamp())\n",
      "  File \"/var/folders/tm/19p9dhs16wx1ggpzkvlwzzc40000gn/T/ipykernel_51442/3276067849.py\", line 21, in extract_timestamp\n",
      "    timestamp_str = event_json['transaction_creation_date']['utc']\n",
      "TypeError: string indices must be integers\n",
      "\n",
      "Exception in thread read_grpc_client_inputs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/data_plane.py\", line 669, in <lambda>\n",
      "    target=lambda: self._read_inputs(elements_iterator),\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/data_plane.py\", line 652, in _read_inputs\n",
      "    for elements in elements_iterator:\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/grpc/_channel.py\", line 540, in __next__\n",
      "    return self._next()\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/grpc/_channel.py\", line 966, in _next\n",
      "    raise self\n",
      "grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n",
      "\tstatus = StatusCode.CANCELLED\n",
      "\tdetails = \"Multiplexer hanging up\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B::1%5D:57164 {created_time:\"2024-02-21T16:53:05.736394+05:30\", grpc_status:1, grpc_message:\"Multiplexer hanging up\"}\"\n",
      ">\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: An error occurred while calling o209.execute.\n",
      ": org.apache.flink.runtime.client.JobExecutionException: Job execution failed.\n",
      "\tat org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)\n",
      "\tat org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:141)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2143)\n",
      "\tat org.apache.flink.runtime.rpc.pekko.PekkoInvocationHandler.lambda$invokeRpc$1(PekkoInvocationHandler.java:268)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2143)\n",
      "\tat org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1267)\n",
      "\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)\n",
      "\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)\n",
      "\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2143)\n",
      "\tat org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$1.onComplete(ScalaFutureUtils.java:47)\n",
      "\tat org.apache.pekko.dispatch.OnComplete.internal(Future.scala:310)\n",
      "\tat org.apache.pekko.dispatch.OnComplete.internal(Future.scala:307)\n",
      "\tat org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:234)\n",
      "\tat org.apache.pekko.dispatch.japi$CallbackBridge.apply(Future.scala:231)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.flink.runtime.concurrent.pekko.ScalaFutureUtils$DirectExecutionContext.execute(ScalaFutureUtils.java:65)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat org.apache.pekko.pattern.PromiseActorRef.$bang(AskSupport.scala:629)\n",
      "\tat org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:34)\n",
      "\tat org.apache.pekko.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:33)\n",
      "\tat scala.concurrent.Future.$anonfun$andThen$1(Future.scala:536)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.pekko.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:73)\n",
      "\tat org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:110)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat org.apache.pekko.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:110)\n",
      "\tat org.apache.pekko.dispatch.TaskInvocation.run(AbstractDispatcher.scala:59)\n",
      "\tat org.apache.pekko.dispatch.ForkJoinExecutorConfigurator$PekkoForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:57)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:295)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1016)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1665)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1598)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
      "Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy\n",
      "\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:176)\n",
      "\tat org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:107)\n",
      "\tat org.apache.flink.runtime.scheduler.DefaultScheduler.recordTaskFailure(DefaultScheduler.java:285)\n",
      "\tat org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:276)\n",
      "\tat org.apache.flink.runtime.scheduler.DefaultScheduler.onTaskFailed(DefaultScheduler.java:269)\n",
      "\tat org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:764)\n",
      "\tat org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:741)\n",
      "\tat org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:83)\n",
      "\tat org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:488)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\n",
      "\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.lambda$handleRpcInvocation$1(PekkoRpcActor.java:309)\n",
      "\tat org.apache.flink.runtime.concurrent.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)\n",
      "\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcInvocation(PekkoRpcActor.java:307)\n",
      "\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleRpcMessage(PekkoRpcActor.java:222)\n",
      "\tat org.apache.flink.runtime.rpc.pekko.FencedPekkoRpcActor.handleRpcMessage(FencedPekkoRpcActor.java:85)\n",
      "\tat org.apache.flink.runtime.rpc.pekko.PekkoRpcActor.handleMessage(PekkoRpcActor.java:168)\n",
      "\tat org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:33)\n",
      "\tat org.apache.pekko.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:29)\n",
      "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
      "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
      "\tat org.apache.pekko.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:29)\n",
      "\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175)\n",
      "\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n",
      "\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n",
      "\tat org.apache.pekko.actor.Actor.aroundReceive(Actor.scala:547)\n",
      "\tat org.apache.pekko.actor.Actor.aroundReceive$(Actor.scala:545)\n",
      "\tat org.apache.pekko.actor.AbstractActor.aroundReceive(AbstractActor.scala:229)\n",
      "\tat org.apache.pekko.actor.ActorCell.receiveMessage(ActorCell.scala:590)\n",
      "\tat org.apache.pekko.actor.ActorCell.invoke(ActorCell.scala:557)\n",
      "\tat org.apache.pekko.dispatch.Mailbox.processMailbox(Mailbox.scala:280)\n",
      "\tat org.apache.pekko.dispatch.Mailbox.run(Mailbox.scala:241)\n",
      "\tat org.apache.pekko.dispatch.Mailbox.exec(Mailbox.scala:253)\n",
      "\t... 5 more\n",
      "Caused by: org.apache.flink.runtime.taskmanager.AsynchronousException: Caught exception while processing timer.\n",
      "\tat org.apache.flink.streaming.runtime.tasks.StreamTask$StreamTaskAsyncExceptionHandler.handleAsyncException(StreamTask.java:1605)\n",
      "\tat org.apache.flink.streaming.runtime.tasks.StreamTask.handleAsyncException(StreamTask.java:1580)\n",
      "\tat org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1734)\n",
      "\tat org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$null$24(StreamTask.java:1723)\n",
      "\tat org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:93)\n",
      "\tat org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)\n",
      "\tat org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398)\n",
      "\tat org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:367)\n",
      "\tat org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)\n",
      "\tat org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)\n",
      "\tat org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:858)\n",
      "\tat org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:807)\n",
      "\tat org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:953)\n",
      "\tat org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:932)\n",
      "\tat org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)\n",
      "\tat org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:831)\n",
      "Caused by: TimerException{java.lang.RuntimeException: Error while waiting for BeamPythonFunctionRunner flush}\n",
      "\t... 15 more\n",
      "Caused by: java.lang.RuntimeException: Error while waiting for BeamPythonFunctionRunner flush\n",
      "\tat org.apache.flink.streaming.api.operators.python.process.AbstractExternalPythonFunctionOperator.invokeFinishBundle(AbstractExternalPythonFunctionOperator.java:107)\n",
      "\tat org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.checkInvokeFinishBundleByTime(AbstractPythonFunctionOperator.java:300)\n",
      "\tat org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.lambda$open$0(AbstractPythonFunctionOperator.java:118)\n",
      "\tat org.apache.flink.streaming.runtime.tasks.StreamTask.invokeProcessingTimeCallback(StreamTask.java:1732)\n",
      "\t... 14 more\n",
      "Caused by: java.lang.RuntimeException: Failed to close remote bundle\n",
      "\tat org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:423)\n",
      "\tat org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.flush(BeamPythonFunctionRunner.java:407)\n",
      "\tat org.apache.flink.streaming.api.operators.python.process.AbstractExternalPythonFunctionOperator.lambda$invokeFinishBundle$0(AbstractExternalPythonFunctionOperator.java:86)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\t... 1 more\n",
      "Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 295, in _execute\n",
      "    response = task()\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 370, in <lambda>\n",
      "    lambda: self.create_worker().do_instruction(request), request)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 629, in do_instruction\n",
      "    return getattr(self, request_type)(\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 667, in process_bundle\n",
      "    bundle_processor.process_bundle(instruction_id))\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 1061, in process_bundle\n",
      "    input_op_by_transform_id[element.transform_id].process_encoded(\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 231, in process_encoded\n",
      "    self.output(decoded_value)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py\", line 528, in output\n",
      "    _cast_to_receiver(self.receivers[output_index]).receive(windowed_value)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py\", line 240, in receive\n",
      "    self.consumer.process(windowed_value)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/beam/beam_operations_slow.py\", line 151, in process\n",
      "    self._main_output_processor.process_outputs(\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/beam/beam_operations_slow.py\", line 65, in process_outputs\n",
      "    self._consumer.process(windowed_value.with_value(results))\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/beam/beam_operations_slow.py\", line 150, in process\n",
      "    for value in o.value:\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/datastream/process/operations.py\", line 197, in wrapped_func\n",
      "    yield from _emit_results(timestamp, watermark, results, has_side_output)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/datastream/process/input_handler.py\", line 131, in _emit_results\n",
      "    for result in results:\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/datastream/data_stream.py\", line 686, in process_element\n",
      "    yield value, self._extract_timestamp_func(value, ctx.timestamp())\n",
      "  File \"/var/folders/tm/19p9dhs16wx1ggpzkvlwzzc40000gn/T/ipykernel_51442/3276067849.py\", line 21, in extract_timestamp\n",
      "    timestamp_str = event_json['transaction_creation_date']['utc']\n",
      "TypeError: string indices must be integers\n",
      "\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2069)\n",
      "\tat org.apache.beam.sdk.util.MoreFutures.get(MoreFutures.java:61)\n",
      "\tat org.apache.beam.runners.fnexecution.control.SdkHarnessClient$BundleProcessor$ActiveBundle.close(SdkHarnessClient.java:504)\n",
      "\tat org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory$1.close(DefaultJobBundleFactory.java:607)\n",
      "\tat org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:421)\n",
      "\t... 7 more\n",
      "Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 295, in _execute\n",
      "    response = task()\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 370, in <lambda>\n",
      "    lambda: self.create_worker().do_instruction(request), request)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 629, in do_instruction\n",
      "    return getattr(self, request_type)(\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 667, in process_bundle\n",
      "    bundle_processor.process_bundle(instruction_id))\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 1061, in process_bundle\n",
      "    input_op_by_transform_id[element.transform_id].process_encoded(\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 231, in process_encoded\n",
      "    self.output(decoded_value)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py\", line 528, in output\n",
      "    _cast_to_receiver(self.receivers[output_index]).receive(windowed_value)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/operations.py\", line 240, in receive\n",
      "    self.consumer.process(windowed_value)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/beam/beam_operations_slow.py\", line 151, in process\n",
      "    self._main_output_processor.process_outputs(\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/beam/beam_operations_slow.py\", line 65, in process_outputs\n",
      "    self._consumer.process(windowed_value.with_value(results))\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/beam/beam_operations_slow.py\", line 150, in process\n",
      "    for value in o.value:\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/datastream/process/operations.py\", line 197, in wrapped_func\n",
      "    yield from _emit_results(timestamp, watermark, results, has_side_output)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/fn_execution/datastream/process/input_handler.py\", line 131, in _emit_results\n",
      "    for result in results:\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/datastream/data_stream.py\", line 686, in process_element\n",
      "    yield value, self._extract_timestamp_func(value, ctx.timestamp())\n",
      "  File \"/var/folders/tm/19p9dhs16wx1ggpzkvlwzzc40000gn/T/ipykernel_51442/3276067849.py\", line 21, in extract_timestamp\n",
      "    timestamp_str = event_json['transaction_creation_date']['utc']\n",
      "TypeError: string indices must be integers\n",
      "\n",
      "\tat org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:180)\n",
      "\tat org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:160)\n",
      "\tat org.apache.beam.vendor.grpc.v1p48p1.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:262)\n",
      "\tat org.apache.beam.vendor.grpc.v1p48p1.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)\n",
      "\tat org.apache.beam.vendor.grpc.v1p48p1.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)\n",
      "\tat org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:332)\n",
      "\tat org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:315)\n",
      "\tat org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:834)\n",
      "\tat org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n",
      "\tat org.apache.beam.vendor.grpc.v1p48p1.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n",
      "\t... 3 more\n",
      "\n",
      "Stopping Consumer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/data_plane.py\", line 505, in input_elements\n",
      "    element = received.get(timeout=1)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/queue.py\", line 179, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 295, in _execute\n",
      "    response = task()\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 370, in <lambda>\n",
      "    lambda: self.create_worker().do_instruction(request), request)\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 629, in do_instruction\n",
      "    return getattr(self, request_type)(\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/sdk_worker.py\", line 667, in process_bundle\n",
      "    bundle_processor.process_bundle(instruction_id))\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/bundle_processor.py\", line 1050, in process_bundle\n",
      "    for element in data_channel.input_elements(instruction_id,\n",
      "  File \"/Users/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/apache_beam/runners/worker/data_plane.py\", line 508, in input_elements\n",
      "    raise RuntimeError('Channel closed prematurely.')\n",
      "RuntimeError: Channel closed prematurely.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#flink-job | basic-watermarking-stream.py\n",
    "#Third Approach\n",
    "#Consume Kafka Events From Topic \n",
    "#using pyflink\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pyflink.common import Types, WatermarkStrategy, Time, Encoder\n",
    "from pyflink.common.watermark_strategy import TimestampAssigner\n",
    "from pyflink.common.serialization import SimpleStringSchema\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors import FlinkKafkaConsumer\n",
    "from pyflink.datastream import TimeCharacteristic\n",
    "\n",
    "\n",
    "class MyTimestampAssigner(TimestampAssigner):\n",
    "\n",
    "    def extract_timestamp(self, value, record_timestamp: int) -> int:\n",
    "        event_json = json.loads(value)\n",
    "        timestamp_str = event_json['transaction_creation_date']['utc']\n",
    "        timestamp_obj = datetime.strptime(timestamp_str, \"%Y-%m-%dT%H:%M:%S\")\n",
    "        timestamp_int = int(timestamp_obj.timestamp())\n",
    "        return timestamp_int\n",
    "\n",
    "\n",
    "def main(topic, group_id):\n",
    "    try:\n",
    "        env = StreamExecutionEnvironment.get_execution_environment()\n",
    "        env.set_parallelism(1)\n",
    "        kafka_properties = {\n",
    "            'bootstrap.servers': 'localhost:9092,localhost:9093,local:9094',\n",
    "            'group.id': group_id\n",
    "        }\n",
    "\n",
    "        kafka_consumer = FlinkKafkaConsumer(\n",
    "            topic,\n",
    "            SimpleStringSchema(),  \n",
    "            kafka_properties)  \n",
    "        \n",
    "        kafka_consumer.set_start_from_earliest()\n",
    "        stream = env.add_source(kafka_consumer)\n",
    "\n",
    "        # Assign timestamps to events and emit watermarks based on event timestamps\n",
    "        stream_with_timestamps = stream.assign_timestamps_and_watermarks(\n",
    "            WatermarkStrategy.for_monotonous_timestamps()\n",
    "                .with_timestamp_assigner(MyTimestampAssigner())\n",
    "        )\n",
    "\n",
    "        def process_event(event):\n",
    "            try:\n",
    "                event_json = json.loads(event)\n",
    "                print('Event Received:\\n' + json.dumps(event_json, indent=4))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print('Error decoding JSON:', e)\n",
    "\n",
    "        stream.map(process_event)\n",
    "        #stream.map(lambda x: print('Event Received:\\n'+json.dumps(json.loads(x), indent=4)))\n",
    "        \n",
    "        env.execute(\"Consume Kafka Events\")\n",
    "    except Exception as e:\n",
    "        print('An error occurred:', e)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        print('Stopping Consumer')\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    topic_name = 'dataeng-poc-flink-watermarking'\n",
    "    group_id = 'flink-consumer-group'   \n",
    "    main(topic_name, group_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6> ('MasterCard', 102.49999952316284)\n",
      "7> ('PayPal', 163.29999923706055)\n",
      "7> ('PayPal', 163.29999923706055)\n",
      "7> ('PayPal', 163.29999923706055)\n",
      "7> ('PayPal', 163.29999923706055)\n",
      "6> ('MasterCard', 102.49999952316284)\n",
      "6> ('MasterCard', 102.49999952316284)\n",
      "6> ('MasterCard', 102.49999952316284)\n"
     ]
    }
   ],
   "source": [
    "#flink-job | auth-watermarking.py\n",
    "\n",
    "from pyflink.common import Time, WatermarkStrategy, Duration\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.common.watermark_strategy import TimestampAssigner\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.functions import KeyedProcessFunction, RuntimeContext\n",
    "from pyflink.datastream.state import ValueStateDescriptor, StateTtlConfig\n",
    "from datetime import datetime\n",
    "\n",
    "class Sum(KeyedProcessFunction):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "\n",
    "    def open(self, runtime_context: RuntimeContext):\n",
    "        state_descriptor = ValueStateDescriptor(\"state\", Types.FLOAT())\n",
    "        state_ttl_config = StateTtlConfig \\\n",
    "            .new_builder(Time.seconds(1)) \\\n",
    "            .set_update_type(StateTtlConfig.UpdateType.OnReadAndWrite) \\\n",
    "            .disable_cleanup_in_background() \\\n",
    "            .build()\n",
    "        state_descriptor.enable_time_to_live(state_ttl_config)\n",
    "        self.state = runtime_context.get_state(state_descriptor)\n",
    "\n",
    "    def process_element(self, value, ctx: 'KeyedProcessFunction.Context'):\n",
    "        # retrieve the current count\n",
    "        current = self.state.value()\n",
    "        if current is None:\n",
    "            current = 0\n",
    "\n",
    "        # update the state's count\n",
    "        current += value[2]\n",
    "        self.state.update(current)\n",
    "\n",
    "        # register an event time timer 2 seconds later\n",
    "        ctx.timer_service().register_event_time_timer(ctx.timestamp() + 2000)\n",
    "\n",
    "    def on_timer(self, timestamp: int, ctx: 'KeyedProcessFunction.OnTimerContext'):\n",
    "        yield ctx.get_current_key(), self.state.value()\n",
    "\n",
    "\n",
    "class MyTimestampAssigner(TimestampAssigner):\n",
    "\n",
    "    def extract_timestamp(self, value, record_timestamp: int) -> int:\n",
    "        timestamp_str = value[0]\n",
    "        timestamp_obj = datetime.strptime(timestamp_str, \"%Y-%m-%dT%H:%M:%S\")\n",
    "        timestamp_int = int(timestamp_obj.timestamp())\n",
    "        return timestamp_int\n",
    "\n",
    "\n",
    "def event_timer_timer_demo():\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "\n",
    "    json_collection = [\n",
    "        {\"transaction_creation_date\": \"2024-02-19T08:00:00\", \"acquirer_name\": \"PayPal\",     \"value\": 110.1},\n",
    "        {\"transaction_creation_date\": \"2024-02-19T08:00:02\", \"acquirer_name\": \"MasterCard\", \"value\": 30.2},  # Out-of-order\n",
    "        {\"transaction_creation_date\": \"2024-02-19T08:00:03\", \"acquirer_name\": \"PayPal\",     \"value\": 20.0},  # Out-of-order\n",
    "        {\"transaction_creation_date\": \"2024-02-19T08:00:01\", \"acquirer_name\": \"MasterCard\", \"value\": 53.1},  # Out-of-order\n",
    "        {\"transaction_creation_date\": \"2024-02-19T08:00:04\", \"acquirer_name\": \"PayPal\",     \"value\": 13.1},  # In-order\n",
    "        {\"transaction_creation_date\": \"2024-02-19T08:00:03\", \"acquirer_name\": \"MasterCard\", \"value\": 3.1},  # In-order\n",
    "        {\"transaction_creation_date\": \"2024-02-19T08:00:07\", \"acquirer_name\": \"MasterCard\", \"value\": 16.1},  # In-order\n",
    "        {\"transaction_creation_date\": \"2024-02-19T08:00:05\", \"acquirer_name\": \"PayPal\",     \"value\": 20.1}  # In-order\n",
    "    ]\n",
    "\n",
    "\n",
    "    def json_to_tuple(json_data):\n",
    "        return (json_data[\"transaction_creation_date\"], json_data[\"acquirer_name\"], json_data[\"value\"])\n",
    "\n",
    "    tuple_collection = [json_to_tuple(json_obj) for json_obj in json_collection]\n",
    "\n",
    "    ds = env.from_collection(tuple_collection, type_info=Types.TUPLE([\n",
    "                            Types.STRING(),\n",
    "                            Types.STRING(),\n",
    "                            Types.FLOAT()\n",
    "                        ]))\n",
    "\n",
    "    ds = ds.assign_timestamps_and_watermarks(\n",
    "        WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(2))\n",
    "                         .with_timestamp_assigner(MyTimestampAssigner()))\n",
    "\n",
    "    # apply the process function onto a keyed stream\n",
    "    ds.key_by(lambda value: value[1]) \\\n",
    "      .process(Sum()) \\\n",
    "      .print()\n",
    "\n",
    "    # submit for execution\n",
    "    env.execute()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    event_timer_timer_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing result to stdout. Use --output to specify output path.\n",
      "(authorizationCreated,0,5,2)\n",
      "(schemesResponded,0,5,2)\n",
      "(schemesResponded,5,10,2)\n",
      "(authorizationCreated,5,10,1)\n",
      "(acquirerResponded,10,15,2)\n",
      "(acquirerRequested,15,20,2)\n",
      "(acquirerResponded,15,20,1)\n"
     ]
    }
   ],
   "source": [
    "#flink-job | auth-wm-tumbling-window.py\n",
    "import sys\n",
    "\n",
    "import argparse\n",
    "from typing import Iterable\n",
    "\n",
    "from pyflink.datastream.connectors.file_system import FileSink, OutputFileConfig, RollingPolicy\n",
    "\n",
    "from pyflink.common import Types, WatermarkStrategy, Time, Encoder\n",
    "from pyflink.common.watermark_strategy import TimestampAssigner\n",
    "from pyflink.datastream import StreamExecutionEnvironment, ProcessWindowFunction\n",
    "from pyflink.datastream.window import TumblingEventTimeWindows, TimeWindow\n",
    "\n",
    "\n",
    "class MyTimestampAssigner(TimestampAssigner):\n",
    "    def extract_timestamp(self, value, record_timestamp) -> int:\n",
    "        return int(value[1])\n",
    "\n",
    "\n",
    "class CountWindowProcessFunction(ProcessWindowFunction[tuple, tuple, str, TimeWindow]):\n",
    "    def process(self,\n",
    "                key: str,\n",
    "                context: ProcessWindowFunction.Context[TimeWindow],\n",
    "                elements: Iterable[tuple]) -> Iterable[tuple]:\n",
    "        return [(key, context.window().start, context.window().end, len([e for e in elements]))]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--output',\n",
    "        dest='output',\n",
    "        required=False,\n",
    "        help='Output file to write results to.')\n",
    "\n",
    "    argv = sys.argv[1:]\n",
    "    known_args, _ = parser.parse_known_args(argv)\n",
    "    output_path = known_args.output\n",
    "\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    # write all the data to one file\n",
    "    env.set_parallelism(1)\n",
    "\n",
    "    # define the source\n",
    "    json_collection = [\n",
    "        { \"event_name\": \"authorizationCreated\",     \"event_order\": 1},\n",
    "        { \"event_name\": \"schemesResponded\", \"event_order\": 2},  # Out-of-order\n",
    "        { \"event_name\": \"authorizationCreated\",     \"event_order\": 3},  # Out-of-order\n",
    "        { \"event_name\": \"schemesResponded\", \"event_order\": 4},  # Out-of-order\n",
    "        { \"event_name\": \"authorizationCreated\",     \"event_order\": 5},  # In-order\n",
    "        { \"event_name\": \"schemesResponded\", \"event_order\": 8},  # In-order\n",
    "        { \"event_name\": \"schemesResponded\", \"event_order\": 9},  # In-order\n",
    "        { \"event_name\": \"acquirerRequested\",     \"event_order\": 19}, # In-order\n",
    "        { \"event_name\": \"acquirerResponded\",     \"event_order\": 18},  # In-order\n",
    "        { \"event_name\": \"acquirerRequested\",     \"event_order\": 16},  # In-order\n",
    "        { \"event_name\": \"acquirerResponded\",     \"event_order\": 12},  # In-order\n",
    "        { \"event_name\": \"acquirerResponded\",     \"event_order\": 11}\n",
    "    ]\n",
    "\n",
    "\n",
    "    def json_to_tuple(json_data):\n",
    "        return (json_data[\"event_name\"], json_data[\"event_order\"])\n",
    "\n",
    "    tuple_collection = [json_to_tuple(json_obj) for json_obj in json_collection]\n",
    "\n",
    "    # define the watermark strategy\n",
    "    watermark_strategy = WatermarkStrategy.for_monotonous_timestamps() \\\n",
    "        .with_timestamp_assigner(MyTimestampAssigner())\n",
    "    \n",
    "    ds = env.from_collection(tuple_collection, type_info=Types.TUPLE([\n",
    "                            Types.STRING(),\n",
    "                            Types.INT()\n",
    "                        ]))\n",
    "\n",
    "    ds = ds.assign_timestamps_and_watermarks(watermark_strategy) \\\n",
    "        .key_by(lambda x: x[0], key_type=Types.STRING()) \\\n",
    "        .window(TumblingEventTimeWindows.of(Time.milliseconds(5))) \\\n",
    "        .process(CountWindowProcessFunction(),\n",
    "                 Types.TUPLE([Types.STRING(), Types.INT(), Types.INT(), Types.INT()]))\n",
    "\n",
    "    # define the sink\n",
    "    if output_path is not None:\n",
    "        ds.sink_to(\n",
    "            sink=FileSink.for_row_format(\n",
    "                base_path=output_path,\n",
    "                encoder=Encoder.simple_string_encoder())\n",
    "            .with_output_file_config(\n",
    "                OutputFileConfig.builder()\n",
    "                .with_part_prefix(\"prefix\")\n",
    "                .with_part_suffix(\".ext\")\n",
    "                .build())\n",
    "            .with_rolling_policy(RollingPolicy.default_rolling_policy())\n",
    "            .build()\n",
    "        )\n",
    "    else:\n",
    "        print(\"Printing result to stdout. Use --output to specify output path.\")\n",
    "        ds.print()\n",
    "\n",
    "    # submit for execution\n",
    "    env.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o72.sqlQuery.\n: org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.kafka_source'.\n\nTable options are:\n\n'connector'='kafka'\n'format'='json'\n'properties.bootstrap.servers'='localhost:9092'\n'properties.group.id'='my_consumer_group'\n'scan.startup.mode'='earliest-offset'\n'topic'='dataeng-poc-flink-watermarking'\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:219)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:244)\n\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.createDynamicTableSource(CatalogSourceTable.java:175)\n\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:115)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:4002)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2872)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2432)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2346)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2291)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:728)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:714)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3848)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2458)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2346)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2291)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:728)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:714)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3848)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:618)\n\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:229)\n\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:205)\n\tat org.apache.flink.table.planner.operations.SqlNodeConvertContext.toRelRoot(SqlNodeConvertContext.java:69)\n\tat org.apache.flink.table.planner.operations.converters.SqlQueryConverter.convertSqlNode(SqlQueryConverter.java:48)\n\tat org.apache.flink.table.planner.operations.converters.SqlNodeConverters.convertSqlNode(SqlNodeConverters.java:73)\n\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:272)\n\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convert(SqlNodeToOperationConversion.java:262)\n\tat org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:708)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:831)\nCaused by: org.apache.flink.table.api.ValidationException: Cannot discover a connector using option: 'connector'='kafka'\n\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:798)\n\tat org.apache.flink.table.factories.FactoryUtil.discoverTableFactory(FactoryUtil.java:772)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:215)\n\t... 38 more\nCaused by: org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'kafka' that implements 'org.apache.flink.table.factories.DynamicTableFactory' in the classpath.\n\nAvailable factory identifiers are:\n\nblackhole\ndatagen\nfilesystem\nprint\npython-input-format\n\tat org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:608)\n\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:794)\n\t... 40 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 59\u001b[0m\n\u001b[1;32m     45\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124m    SELECT\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124m        TUMBLE_START(transaction_creation_date_utc, INTERVAL \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m MINUTE) AS window_start,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124m    GROUP BY TUMBLE(transaction_creation_date_utc, INTERVAL \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m MINUTE)\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Execute the query\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m result_table \u001b[38;5;241m=\u001b[39m \u001b[43mt_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Convert the result table to a DataStream and print the results\u001b[39;00m\n\u001b[1;32m     62\u001b[0m result_table\u001b[38;5;241m.\u001b[39mexecute_and_print()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/table/table_environment.py:819\u001b[0m, in \u001b[0;36mTableEnvironment.sql_query\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Table:\n\u001b[1;32m    800\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;124;03m    Evaluates a SQL query on registered tables and retrieves the result as a\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;124;03m    :class:`~pyflink.table.Table`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;124;03m    :return: The result table.\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 819\u001b[0m     j_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_j_tenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqlQuery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Table(j_table, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/util/exceptions.py:146\u001b[0m, in \u001b[0;36mcapture_java_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyflink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_gateway\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_gateway\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o72.sqlQuery.\n: org.apache.flink.table.api.ValidationException: Unable to create a source for reading table 'default_catalog.default_database.kafka_source'.\n\nTable options are:\n\n'connector'='kafka'\n'format'='json'\n'properties.bootstrap.servers'='localhost:9092'\n'properties.group.id'='my_consumer_group'\n'scan.startup.mode'='earliest-offset'\n'topic'='dataeng-poc-flink-watermarking'\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:219)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:244)\n\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.createDynamicTableSource(CatalogSourceTable.java:175)\n\tat org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.java:115)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:4002)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2872)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2432)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2346)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2291)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:728)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:714)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3848)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2458)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2346)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2291)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:728)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:714)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3848)\n\tat org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:618)\n\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:229)\n\tat org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:205)\n\tat org.apache.flink.table.planner.operations.SqlNodeConvertContext.toRelRoot(SqlNodeConvertContext.java:69)\n\tat org.apache.flink.table.planner.operations.converters.SqlQueryConverter.convertSqlNode(SqlQueryConverter.java:48)\n\tat org.apache.flink.table.planner.operations.converters.SqlNodeConverters.convertSqlNode(SqlNodeConverters.java:73)\n\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convertValidatedSqlNode(SqlNodeToOperationConversion.java:272)\n\tat org.apache.flink.table.planner.operations.SqlNodeToOperationConversion.convert(SqlNodeToOperationConversion.java:262)\n\tat org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:708)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:831)\nCaused by: org.apache.flink.table.api.ValidationException: Cannot discover a connector using option: 'connector'='kafka'\n\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:798)\n\tat org.apache.flink.table.factories.FactoryUtil.discoverTableFactory(FactoryUtil.java:772)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSource(FactoryUtil.java:215)\n\t... 38 more\nCaused by: org.apache.flink.table.api.ValidationException: Could not find any factory for identifier 'kafka' that implements 'org.apache.flink.table.factories.DynamicTableFactory' in the classpath.\n\nAvailable factory identifiers are:\n\nblackhole\ndatagen\nfilesystem\nprint\npython-input-format\n\tat org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:608)\n\tat org.apache.flink.table.factories.FactoryUtil.enrichNoMatchingConnectorError(FactoryUtil.java:794)\n\t... 40 more\n"
     ]
    }
   ],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment, DataTypes\n",
    "from pyflink.table.udf import udf\n",
    "from json import loads\n",
    "\n",
    "# Kafka consumer configuration\n",
    "kafka_props = {\n",
    "    'bootstrap.servers': 'localhost:9092,localhost:9093,local:9094',\n",
    "    'group.id': 'my_consumer_group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}\n",
    "\n",
    "# Define the watermark generation function\n",
    "@udf(result_type=DataTypes.TIMESTAMP(3))\n",
    "def assign_watermark(ts):\n",
    "    # Add 5 seconds of allowed lateness as watermark\n",
    "    return ts - 5000  # in milliseconds\n",
    "\n",
    "# Set up Flink environment\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.set_parallelism(1)  # Set parallelism to 1 for demonstration purpose\n",
    "t_env = StreamTableEnvironment.create(env)\n",
    "\n",
    "# Register watermark UDF\n",
    "t_env.register_function(\"assign_watermark\", assign_watermark)\n",
    "\n",
    "# Kafka source table\n",
    "kafka_source_ddl = f\"\"\"\n",
    "    CREATE TABLE kafka_source (\n",
    "        message STRING\n",
    "    ) WITH (\n",
    "        'connector' = 'kafka',\n",
    "        'topic' = 'dataeng-poc-flink-watermarking',\n",
    "        'properties.bootstrap.servers' = '{kafka_props['bootstrap.servers']}',\n",
    "        'properties.group.id' = '{kafka_props['group.id']}',\n",
    "        'format' = 'json',\n",
    "        'scan.startup.mode' = 'earliest-offset'\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "# Register Kafka source table\n",
    "t_env.execute_sql(kafka_source_ddl)\n",
    "\n",
    "# Define the query to process the data with watermarking\n",
    "query = f\"\"\"\n",
    "    SELECT\n",
    "        TUMBLE_START(transaction_creation_date_utc, INTERVAL '1' MINUTE) AS window_start,\n",
    "        TUMBLE_END(transaction_creation_date_utc, INTERVAL '1' MINUTE) AS window_end,\n",
    "        COUNT(*) AS event_count\n",
    "    FROM (\n",
    "        SELECT\n",
    "            CAST(JSON_VALUE(message, '$.transaction_creation_date.utc') AS TIMESTAMP) AS transaction_creation_date_utc\n",
    "        FROM kafka_source\n",
    "    )\n",
    "    GROUP BY TUMBLE(transaction_creation_date_utc, INTERVAL '1' MINUTE)\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result_table = t_env.sql_query(query)\n",
    "\n",
    "# Convert the result table to a DataStream and print the results\n",
    "result_table.execute_and_print()\n",
    "\n",
    "# Start the execution\n",
    "env.execute(\"Flink Watermarking Example\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o24.executeSql.\n: org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered \"value\" at line 3, column 9.\nWas expecting one of:\n    \"CONSTRAINT\" ...\n    \"PRIMARY\" ...\n    \"UNIQUE\" ...\n    \"WATERMARK\" ...\n    <BRACKET_QUOTED_IDENTIFIER> ...\n    <QUOTED_IDENTIFIER> ...\n    <BACK_QUOTED_IDENTIFIER> ...\n    <BIG_QUERY_BACK_QUOTED_IDENTIFIER> ...\n    <HYPHENATED_IDENTIFIER> ...\n    <IDENTIFIER> ...\n    <UNICODE_QUOTED_IDENTIFIER> ...\n    \n\tat org.apache.flink.table.planner.parse.CalciteParser.parseSqlList(CalciteParser.java:81)\n\tat org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:102)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:728)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:831)\nCaused by: org.apache.calcite.sql.parser.SqlParseException: Encountered \"value\" at line 3, column 9.\nWas expecting one of:\n    \"CONSTRAINT\" ...\n    \"PRIMARY\" ...\n    \"UNIQUE\" ...\n    \"WATERMARK\" ...\n    <BRACKET_QUOTED_IDENTIFIER> ...\n    <QUOTED_IDENTIFIER> ...\n    <BACK_QUOTED_IDENTIFIER> ...\n    <BIG_QUERY_BACK_QUOTED_IDENTIFIER> ...\n    <HYPHENATED_IDENTIFIER> ...\n    <IDENTIFIER> ...\n    <UNICODE_QUOTED_IDENTIFIER> ...\n    \n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.convertException(FlinkSqlParserImpl.java:512)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.normalizeException(FlinkSqlParserImpl.java:265)\n\tat org.apache.calcite.sql.parser.SqlParser.handleException(SqlParser.java:156)\n\tat org.apache.calcite.sql.parser.SqlParser.parseStmtList(SqlParser.java:211)\n\tat org.apache.flink.table.planner.parse.CalciteParser.parseSqlList(CalciteParser.java:76)\n\t... 13 more\nCaused by: org.apache.flink.sql.parser.impl.ParseException: Encountered \"value\" at line 3, column 9.\nWas expecting one of:\n    \"CONSTRAINT\" ...\n    \"PRIMARY\" ...\n    \"UNIQUE\" ...\n    \"WATERMARK\" ...\n    <BRACKET_QUOTED_IDENTIFIER> ...\n    <QUOTED_IDENTIFIER> ...\n    <BACK_QUOTED_IDENTIFIER> ...\n    <BIG_QUERY_BACK_QUOTED_IDENTIFIER> ...\n    <HYPHENATED_IDENTIFIER> ...\n    <IDENTIFIER> ...\n    <UNICODE_QUOTED_IDENTIFIER> ...\n    \n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.generateParseException(FlinkSqlParserImpl.java:48939)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.jj_consume_token(FlinkSqlParserImpl.java:48747)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.TableColumn(FlinkSqlParserImpl.java:7550)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlCreateTable(FlinkSqlParserImpl.java:8759)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlCreateExtended(FlinkSqlParserImpl.java:9996)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlCreate(FlinkSqlParserImpl.java:27604)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmt(FlinkSqlParserImpl.java:3781)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmtList(FlinkSqlParserImpl.java:3154)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.parseSqlStmtList(FlinkSqlParserImpl.java:321)\n\tat org.apache.calcite.sql.parser.SqlParser.parseStmtList(SqlParser.java:209)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m\n\u001b[1;32m     20\u001b[0m kafka_source_ddl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m    CREATE TABLE kafka_source (\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m        value STRING\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m    )\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Register the Kafka source table\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mt_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkafka_source_ddl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Define your query\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Here, I'm simply selecting and printing the values from the Kafka source table\u001b[39;00m\n\u001b[1;32m     37\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m    SELECT *\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124m    FROM kafka_source\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/table/table_environment.py:837\u001b[0m, in \u001b[0;36mTableEnvironment.execute_sql\u001b[0;34m(self, stmt)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03mExecute the given single statement, and return the execution result.\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.11.0\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_execute()\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TableResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_j_tenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuteSql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pyflink/util/exceptions.py:146\u001b[0m, in \u001b[0;36mcapture_java_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyflink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_gateway\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_gateway\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.executeSql.\n: org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered \"value\" at line 3, column 9.\nWas expecting one of:\n    \"CONSTRAINT\" ...\n    \"PRIMARY\" ...\n    \"UNIQUE\" ...\n    \"WATERMARK\" ...\n    <BRACKET_QUOTED_IDENTIFIER> ...\n    <QUOTED_IDENTIFIER> ...\n    <BACK_QUOTED_IDENTIFIER> ...\n    <BIG_QUERY_BACK_QUOTED_IDENTIFIER> ...\n    <HYPHENATED_IDENTIFIER> ...\n    <IDENTIFIER> ...\n    <UNICODE_QUOTED_IDENTIFIER> ...\n    \n\tat org.apache.flink.table.planner.parse.CalciteParser.parseSqlList(CalciteParser.java:81)\n\tat org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:102)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:728)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:831)\nCaused by: org.apache.calcite.sql.parser.SqlParseException: Encountered \"value\" at line 3, column 9.\nWas expecting one of:\n    \"CONSTRAINT\" ...\n    \"PRIMARY\" ...\n    \"UNIQUE\" ...\n    \"WATERMARK\" ...\n    <BRACKET_QUOTED_IDENTIFIER> ...\n    <QUOTED_IDENTIFIER> ...\n    <BACK_QUOTED_IDENTIFIER> ...\n    <BIG_QUERY_BACK_QUOTED_IDENTIFIER> ...\n    <HYPHENATED_IDENTIFIER> ...\n    <IDENTIFIER> ...\n    <UNICODE_QUOTED_IDENTIFIER> ...\n    \n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.convertException(FlinkSqlParserImpl.java:512)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.normalizeException(FlinkSqlParserImpl.java:265)\n\tat org.apache.calcite.sql.parser.SqlParser.handleException(SqlParser.java:156)\n\tat org.apache.calcite.sql.parser.SqlParser.parseStmtList(SqlParser.java:211)\n\tat org.apache.flink.table.planner.parse.CalciteParser.parseSqlList(CalciteParser.java:76)\n\t... 13 more\nCaused by: org.apache.flink.sql.parser.impl.ParseException: Encountered \"value\" at line 3, column 9.\nWas expecting one of:\n    \"CONSTRAINT\" ...\n    \"PRIMARY\" ...\n    \"UNIQUE\" ...\n    \"WATERMARK\" ...\n    <BRACKET_QUOTED_IDENTIFIER> ...\n    <QUOTED_IDENTIFIER> ...\n    <BACK_QUOTED_IDENTIFIER> ...\n    <BIG_QUERY_BACK_QUOTED_IDENTIFIER> ...\n    <HYPHENATED_IDENTIFIER> ...\n    <IDENTIFIER> ...\n    <UNICODE_QUOTED_IDENTIFIER> ...\n    \n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.generateParseException(FlinkSqlParserImpl.java:48939)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.jj_consume_token(FlinkSqlParserImpl.java:48747)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.TableColumn(FlinkSqlParserImpl.java:7550)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlCreateTable(FlinkSqlParserImpl.java:8759)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlCreateExtended(FlinkSqlParserImpl.java:9996)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlCreate(FlinkSqlParserImpl.java:27604)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmt(FlinkSqlParserImpl.java:3781)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.SqlStmtList(FlinkSqlParserImpl.java:3154)\n\tat org.apache.flink.sql.parser.impl.FlinkSqlParserImpl.parseSqlStmtList(FlinkSqlParserImpl.java:321)\n\tat org.apache.calcite.sql.parser.SqlParser.parseStmtList(SqlParser.java:209)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment, EnvironmentSettings\n",
    "\n",
    "# Set up the environment\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.set_parallelism(1)  # You can adjust the parallelism as needed\n",
    "t_env = StreamTableEnvironment.create(env)\n",
    "\n",
    "# Define the Kafka source properties\n",
    "kafka_props = {\n",
    "    \"bootstrap.servers\": \"localhost:9092,localhost:9093,local:9094\",\n",
    "    \"group.id\": \"flink-consumer-group\",\n",
    "    \"auto.offset.reset\": \"latest\"\n",
    "}\n",
    "\n",
    "# Define the schema for your Kafka topic\n",
    "# Make sure the schema matches the structure of the events in your Kafka topic\n",
    "# Here, I'm assuming a simple schema with a single 'value' field\n",
    "kafka_source_ddl = f\"\"\"\n",
    "    CREATE TABLE kafka_source (\n",
    "        value STRING\n",
    "    ) WITH (\n",
    "        'connector' = 'kafka',\n",
    "        'topic' = 'dataeng-poc-flink-watermarking',\n",
    "        'properties.bootstrap.servers' = '{kafka_props[\"bootstrap.servers\"]}',\n",
    "        'properties.group.id' = '{kafka_props[\"group.id\"]}',\n",
    "        'format' = 'json'\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "# Register the Kafka source table\n",
    "t_env.execute_sql(kafka_source_ddl)\n",
    "\n",
    "# Define your query\n",
    "# Here, I'm simply selecting and printing the values from the Kafka source table\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM kafka_source\n",
    "\"\"\"\n",
    "\n",
    "# Convert the query to a DataStream and print the results\n",
    "t_env.to_append_stream(t_env.sql_query(query)).print()\n",
    "\n",
    "# Execute the job\n",
    "env.execute(\"Consume Kafka events in Flink\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Any for unsupported type: typing.Sequence[~T]\n",
      "No module named google.cloud.bigquery_storage_v1. As a result, the ReadFromBigQuery transform *CANNOT* be used with `method=DIRECT_READ`.\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| op |                   id |                           name |                            tel |                        country |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| +I |                    1 |                          Flink |                            123 |                        Germany |\n",
      "| +I |                    2 |                          hello |                            135 |                          China |\n",
      "| +I |                    3 |                          world |                            124 |                            USA |\n",
      "| +I |                    4 |                        PyFlink |                             32 |                          China |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "4 rows in set\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| op |                   id |                           name |                            tel |                        country |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| +I |                    1 |                          Flink |                            123 |                        Germany |\n",
      "| +I |                    2 |                          hello |                            135 |                          China |\n",
      "| +I |                    3 |                          world |                            124 |                            USA |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "3 rows in set\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| op |                   id |                           name |                            tel |                        country |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| +I |                    1 |                          Flink |                            123 |                        Germany |\n",
      "| +I |                    2 |                          hello |                            135 |                          China |\n",
      "| +I |                    4 |                        PyFlink |                             32 |                          China |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "3 rows in set\n",
      "+----+--------------------------------+----------------------+----------------------+\n",
      "| op |                        country |               EXPR$0 |               EXPR$1 |\n",
      "+----+--------------------------------+----------------------+----------------------+\n",
      "| +I |                            USA |                    1 |                  124 |\n",
      "| +I |                        Germany |                    1 |                  123 |\n",
      "| +I |                          China |                    1 |                  135 |\n",
      "| -U |                          China |                    1 |                  135 |\n",
      "| +U |                          China |                    2 |                  135 |\n",
      "+----+--------------------------------+----------------------+----------------------+\n",
      "5 rows in set\n",
      "+----+--------------------------------+\n",
      "| op |                        country |\n",
      "+----+--------------------------------+\n",
      "| +I |                        Germany |\n",
      "| +I |                          China |\n",
      "| +I |                            USA |\n",
      "+----+--------------------------------+\n",
      "3 rows in set\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+----------------------+----------------------+\n",
      "| op |                   id |                           name |                            tel |                        country |                 r_id |                  age |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+----------------------+----------------------+\n",
      "| +I |                    1 |                          Flink |                            123 |                        Germany |                    1 |                   18 |\n",
      "| +I |                    2 |                          hello |                            135 |                          China |                    2 |                   30 |\n",
      "| +I |                    3 |                          world |                            124 |                            USA |                    3 |                   25 |\n",
      "| +I |                    4 |                        PyFlink |                             32 |                          China |                    4 |                   10 |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+----------------------+----------------------+\n",
      "4 rows in set\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| op |                   id |                           name |                            tel |                        country |                              a |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| +I |                    1 |                          Flink |                            123 |                        Germany |                             Fl |\n",
      "| +I |                    1 |                          Flink |                            123 |                        Germany |                             nk |\n",
      "| +I |                    2 |                          hello |                            135 |                          China |                          hello |\n",
      "| +I |                    3 |                          world |                            124 |                            USA |                          world |\n",
      "| +I |                    4 |                        PyFlink |                             32 |                          China |                           PyFl |\n",
      "| +I |                    4 |                        PyFlink |                             32 |                          China |                             nk |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "6 rows in set\n",
      "(\n",
      "  `id` BIGINT,\n",
      "  `name` STRING,\n",
      "  `tel` STRING,\n",
      "  `country` STRING\n",
      ")\n",
      "== Abstract Syntax Tree ==\n",
      "LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{}])\n",
      ":- LogicalProject(id=[$0], name=[JSON_VALUE($1, _UTF-16LE'$.name', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR))], tel=[JSON_VALUE($1, _UTF-16LE'$.tel', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR))], country=[JSON_VALUE($1, _UTF-16LE'$.addr.country', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR))])\n",
      ":  +- LogicalTableScan(table=[[*anonymous_python-input-format$1*]])\n",
      "+- LogicalTableFunctionScan(invocation=[*org.apache.flink.table.functions.python.PythonTableFunction$663cb0e09f51c115ea59da403c48db6d*($0, $1, $2, $3)], rowType=[RecordType(VARCHAR(2147483647) a)])\n",
      "\n",
      "== Optimized Physical Plan ==\n",
      "PythonCorrelate(invocation=[*org.apache.flink.table.functions.python.PythonTableFunction$663cb0e09f51c115ea59da403c48db6d*($0, $1, $2, $3)], correlate=[table(*org.apache.flink.table.functions.python.PythonTableFunction$663cb0e09f51c115ea59da403c48db6d*(id,name,tel,country))], select=[id,name,tel,country,a], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) name, VARCHAR(2147483647) tel, VARCHAR(2147483647) country, VARCHAR(2147483647) a)], joinType=[INNER])\n",
      "+- Calc(select=[id, JSON_VALUE(data, _UTF-16LE'$.name', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR)) AS name, JSON_VALUE(data, _UTF-16LE'$.tel', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR)) AS tel, JSON_VALUE(data, _UTF-16LE'$.addr.country', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR)) AS country])\n",
      "   +- TableSourceScan(table=[[*anonymous_python-input-format$1*]], fields=[id, data])\n",
      "\n",
      "== Optimized Execution Plan ==\n",
      "PythonCorrelate(invocation=[*org.apache.flink.table.functions.python.PythonTableFunction$663cb0e09f51c115ea59da403c48db6d*($0, $1, $2, $3)], correlate=[table(*org.apache.flink.table.functions.python.PythonTableFunction$663cb0e09f51c115ea59da403c48db6d*(id,name,tel,country))], select=[id,name,tel,country,a], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) name, VARCHAR(2147483647) tel, VARCHAR(2147483647) country, VARCHAR(2147483647) a)], joinType=[INNER])\n",
      "+- Calc(select=[id, JSON_VALUE(data, '$.name', NULL, ON EMPTY, NULL, ON ERROR) AS name, JSON_VALUE(data, '$.tel', NULL, ON EMPTY, NULL, ON ERROR) AS tel, JSON_VALUE(data, '$.addr.country', NULL, ON EMPTY, NULL, ON ERROR) AS country])\n",
      "   +- TableSourceScan(table=[[*anonymous_python-input-format$1*]], fields=[id, data])\n",
      "\n",
      "== Abstract Syntax Tree ==\n",
      "LogicalCorrelate(correlation=[$cor2], joinType=[inner], requiredColumns=[{}])\n",
      ":- LogicalProject(id=[$0], name=[JSON_VALUE($1, _UTF-16LE'$.name', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR))], tel=[JSON_VALUE($1, _UTF-16LE'$.tel', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR))], country=[JSON_VALUE($1, _UTF-16LE'$.addr.country', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR))])\n",
      ":  +- LogicalTableScan(table=[[*anonymous_python-input-format$1*]])\n",
      "+- LogicalTableFunctionScan(invocation=[*org.apache.flink.table.functions.python.PythonTableFunction$663cb0e09f51c115ea59da403c48db6d*($0, $1, $2, $3)], rowType=[RecordType(VARCHAR(2147483647) a)])\n",
      "\n",
      "== Optimized Physical Plan With Advice ==\n",
      "PythonCorrelate(invocation=[*org.apache.flink.table.functions.python.PythonTableFunction$663cb0e09f51c115ea59da403c48db6d*($0, $1, $2, $3)], correlate=[table(*org.apache.flink.table.functions.python.PythonTableFunction$663cb0e09f51c115ea59da403c48db6d*(id,name,tel,country))], select=[id,name,tel,country,a], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) name, VARCHAR(2147483647) tel, VARCHAR(2147483647) country, VARCHAR(2147483647) a)], joinType=[INNER])\n",
      "+- Calc(select=[id, JSON_VALUE(data, _UTF-16LE'$.name', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR)) AS name, JSON_VALUE(data, _UTF-16LE'$.tel', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR)) AS tel, JSON_VALUE(data, _UTF-16LE'$.addr.country', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR)) AS country])\n",
      "   +- TableSourceScan(table=[[*anonymous_python-input-format$1*]], fields=[id, data])\n",
      "\n",
      "No available advice...\n",
      "\n",
      "== Optimized Execution Plan ==\n",
      "PythonCorrelate(invocation=[*org.apache.flink.table.functions.python.PythonTableFunction$663cb0e09f51c115ea59da403c48db6d*($0, $1, $2, $3)], correlate=[table(*org.apache.flink.table.functions.python.PythonTableFunction$663cb0e09f51c115ea59da403c48db6d*(id,name,tel,country))], select=[id,name,tel,country,a], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) name, VARCHAR(2147483647) tel, VARCHAR(2147483647) country, VARCHAR(2147483647) a)], joinType=[INNER])\n",
      "+- Calc(select=[id, JSON_VALUE(data, '$.name', NULL, ON EMPTY, NULL, ON ERROR) AS name, JSON_VALUE(data, '$.tel', NULL, ON EMPTY, NULL, ON ERROR) AS tel, JSON_VALUE(data, '$.addr.country', NULL, ON EMPTY, NULL, ON ERROR) AS country])\n",
      "   +- TableSourceScan(table=[[*anonymous_python-input-format$1*]], fields=[id, data])\n",
      "\n",
      "+----+----------------------+--------------------------------+\n",
      "| op |                   id |                           data |\n",
      "+----+----------------------+--------------------------------+\n",
      "| +I |                    1 | {\"name\": \"Flink\", \"tel\": 12... |\n",
      "| +I |                    2 | {\"name\": \"hello\", \"tel\": 13... |\n",
      "| +I |                    3 | {\"name\": \"world\", \"tel\": 12... |\n",
      "| +I |                    4 | {\"name\": \"PyFlink\", \"tel\": ... |\n",
      "+----+----------------------+--------------------------------+\n",
      "4 rows in set\n",
      "+----+----------------------+--------------------------------+--------------------------------+-------------+--------------------------------+\n",
      "| op |                   id |                           data |                           name |         tel |                        country |\n",
      "+----+----------------------+--------------------------------+--------------------------------+-------------+--------------------------------+\n",
      "| +I |                    1 | {\"name\": \"Flink\", \"tel\": 12... |                          Flink |         123 |                        Germany |\n",
      "| +I |                    2 | {\"name\": \"hello\", \"tel\": 13... |                          hello |         135 |                          China |\n",
      "| +I |                    3 | {\"name\": \"world\", \"tel\": 12... |                          world |         124 |                            USA |\n",
      "| +I |                    4 | {\"name\": \"PyFlink\", \"tel\": ... |                        PyFlink |          32 |                          China |\n",
      "+----+----------------------+--------------------------------+--------------------------------+-------------+--------------------------------+\n",
      "4 rows in set\n",
      "== Abstract Syntax Tree ==\n",
      "LogicalProject(id=[$0], data=[$1], name=[$2], tel=[$3], country=[$4])\n",
      "+- LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{1}])\n",
      "   :- LogicalTableScan(table=[[*anonymous_python-input-format$10*]])\n",
      "   +- LogicalTableFunctionScan(invocation=[parse_data($cor1.data)], rowType=[RecordType:peek_no_expand(VARCHAR(2147483647) f0, INTEGER f1, VARCHAR(2147483647) f2)])\n",
      "\n",
      "== Optimized Physical Plan ==\n",
      "PythonCorrelate(invocation=[parse_data($1)], correlate=[table(parse_data(data))], select=[id,data,f0,f1,f2], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) data, VARCHAR(2147483647) f0, INTEGER f1, VARCHAR(2147483647) f2)], joinType=[INNER])\n",
      "+- TableSourceScan(table=[[*anonymous_python-input-format$10*]], fields=[id, data])\n",
      "\n",
      "== Optimized Execution Plan ==\n",
      "PythonCorrelate(invocation=[parse_data($1)], correlate=[table(parse_data(data))], select=[id,data,f0,f1,f2], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) data, VARCHAR(2147483647) f0, INTEGER f1, VARCHAR(2147483647) f2)], joinType=[INNER])\n",
      "+- TableSourceScan(table=[[*anonymous_python-input-format$10*]], fields=[id, data])\n",
      "\n",
      "== Abstract Syntax Tree ==\n",
      "LogicalProject(id=[$0], data=[$1], name=[$2], tel=[$3], country=[$4])\n",
      "+- LogicalCorrelate(correlation=[$cor2], joinType=[inner], requiredColumns=[{1}])\n",
      "   :- LogicalTableScan(table=[[*anonymous_python-input-format$10*]])\n",
      "   +- LogicalTableFunctionScan(invocation=[parse_data($cor2.data)], rowType=[RecordType:peek_no_expand(VARCHAR(2147483647) f0, INTEGER f1, VARCHAR(2147483647) f2)])\n",
      "\n",
      "== Optimized Physical Plan With Advice ==\n",
      "PythonCorrelate(invocation=[parse_data($1)], correlate=[table(parse_data(data))], select=[id,data,f0,f1,f2], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) data, VARCHAR(2147483647) f0, INTEGER f1, VARCHAR(2147483647) f2)], joinType=[INNER])\n",
      "+- TableSourceScan(table=[[*anonymous_python-input-format$10*]], fields=[id, data])\n",
      "\n",
      "No available advice...\n",
      "\n",
      "== Optimized Execution Plan ==\n",
      "PythonCorrelate(invocation=[parse_data($1)], correlate=[table(parse_data(data))], select=[id,data,f0,f1,f2], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) data, VARCHAR(2147483647) f0, INTEGER f1, VARCHAR(2147483647) f2)], joinType=[INNER])\n",
      "+- TableSourceScan(table=[[*anonymous_python-input-format$10*]], fields=[id, data])\n",
      "\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| op |                   id |                           data |                           name |                            tel |                        country |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| +I |                    1 | {\"name\": \"Flink\", \"tel\": 12... |                          Flink |                            123 |                        Germany |\n",
      "| +I |                    2 | {\"name\": \"hello\", \"tel\": 13... |                          hello |                            135 |                          China |\n",
      "| +I |                    3 | {\"name\": \"world\", \"tel\": 12... |                          world |                            124 |                            USA |\n",
      "| +I |                    4 | {\"name\": \"PyFlink\", \"tel\": ... |                        PyFlink |                             32 |                          China |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "4 rows in set\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| op |                   id |                           name |                            tel |                        country |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| +I |                    1 |                          Flink |                            123 |                        Germany |\n",
      "| +I |                    2 |                          hello |                            135 |                          China |\n",
      "| +I |                    3 |                          world |                            124 |                            USA |\n",
      "| +I |                    4 |                        PyFlink |                             32 |                          China |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "4 rows in set\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| op |                   id |                           name |                      telephone |                        country |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| +I |                    1 |                          Flink |                            123 |                        Germany |\n",
      "| +I |                    2 |                          hello |                            135 |                          China |\n",
      "| +I |                    3 |                          world |                            124 |                            USA |\n",
      "| +I |                    4 |                        PyFlink |                             32 |                          China |\n",
      "+----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "4 rows in set\n",
      "+----+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| op |                             id |                           name |                      telephone |                        country |\n",
      "+----+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| +I |                        1_Flink |                          Flink |                            123 |                        Germany |\n",
      "| +I |                        2_hello |                          hello |                            135 |                          China |\n",
      "| +I |                        3_world |                          world |                            124 |                            USA |\n",
      "| +I |                      4_PyFlink |                        PyFlink |                             32 |                          China |\n",
      "+----+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "4 rows in set\n",
      "+----+----------------------+--------------------------------+\n",
      "| op |                   id |                        country |\n",
      "+----+----------------------+--------------------------------+\n",
      "| +I |                    1 |                        Germany |\n",
      "| +I |                    2 |                          China |\n",
      "| +I |                    3 |                          China |\n",
      "| +I |                    4 |                          China |\n",
      "+----+----------------------+--------------------------------+\n",
      "4 rows in set\n",
      "+----+----------------------+--------------------------------+\n",
      "| op |                   f0 |                             f1 |\n",
      "+----+----------------------+--------------------------------+\n",
      "| +I |                    1 |                         Berlin |\n",
      "| +I |                    2 |                       Shanghai |\n",
      "| +I |                    3 |                        NewYork |\n",
      "| +I |                    4 |                       Hangzhou |\n",
      "+----+----------------------+--------------------------------+\n",
      "4 rows in set\n",
      "+----+--------------------------------+----------------------+----------------------+\n",
      "| op |                        country |                  cnt |                  sum |\n",
      "+----+--------------------------------+----------------------+----------------------+\n",
      "| +I |                          China |                    3 |                  291 |\n",
      "| +I |                        Germany |                    1 |                  123 |\n",
      "+----+--------------------------------+----------------------+----------------------+\n",
      "2 rows in set\n",
      "+----+--------------------------------+----------------------+\n",
      "| op |                        country |                  tel |\n",
      "+----+--------------------------------+----------------------+\n",
      "| +I |                          China |                  135 |\n",
      "| +I |                          China |                  124 |\n",
      "| +I |                        Germany |                  123 |\n",
      "+----+--------------------------------+----------------------+\n",
      "3 rows in set\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from pyflink.common import Row\n",
    "from pyflink.table import (DataTypes, TableEnvironment, EnvironmentSettings, ExplainDetail)\n",
    "from pyflink.table.expressions import *\n",
    "from pyflink.table.udf import udtf, udf, udaf, AggregateFunction, TableAggregateFunction, udtaf\n",
    "\n",
    "\n",
    "def basic_operations():\n",
    "    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())\n",
    "\n",
    "    # define the source\n",
    "    table = t_env.from_elements(\n",
    "        elements=[\n",
    "            (1, '{\"name\": \"Flink\", \"tel\": 123, \"addr\": {\"country\": \"Germany\", \"city\": \"Berlin\"}}'),\n",
    "            (2, '{\"name\": \"hello\", \"tel\": 135, \"addr\": {\"country\": \"China\", \"city\": \"Shanghai\"}}'),\n",
    "            (3, '{\"name\": \"world\", \"tel\": 124, \"addr\": {\"country\": \"USA\", \"city\": \"NewYork\"}}'),\n",
    "            (4, '{\"name\": \"PyFlink\", \"tel\": 32, \"addr\": {\"country\": \"China\", \"city\": \"Hangzhou\"}}')\n",
    "        ],\n",
    "        schema=['id', 'data'])\n",
    "\n",
    "    right_table = t_env.from_elements(elements=[(1, 18), (2, 30), (3, 25), (4, 10)],\n",
    "                                      schema=['id', 'age'])\n",
    "\n",
    "    table = table.add_columns(\n",
    "                    col('data').json_value('$.name', DataTypes.STRING()).alias('name'),\n",
    "                    col('data').json_value('$.tel', DataTypes.STRING()).alias('tel'),\n",
    "                    col('data').json_value('$.addr.country', DataTypes.STRING()).alias('country')) \\\n",
    "                 .drop_columns(col('data'))\n",
    "    table.execute().print()\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | op |                   id |                           name |                            tel |                        country |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | +I |                    1 |                          Flink |                            123 |                        Germany |\n",
    "    # | +I |                    2 |                          hello |                            135 |                          China |\n",
    "    # | +I |                    3 |                          world |                            124 |                            USA |\n",
    "    # | +I |                    4 |                        PyFlink |                             32 |                          China |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "\n",
    "    # limit the number of outputs\n",
    "    table.limit(3).execute().print()\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | op |                   id |                           name |                            tel |                        country |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | +I |                    1 |                          Flink |                            123 |                        Germany |\n",
    "    # | +I |                    2 |                          hello |                            135 |                          China |\n",
    "    # | +I |                    3 |                          world |                            124 |                            USA |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "\n",
    "    # filter\n",
    "    table.filter(col('id') != 3).execute().print()\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | op |                   id |                           name |                            tel |                        country |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | +I |                    1 |                          Flink |                            123 |                        Germany |\n",
    "    # | +I |                    2 |                          hello |                            135 |                          China |\n",
    "    # | +I |                    4 |                        PyFlink |                             32 |                          China |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "\n",
    "    # aggregation\n",
    "    table.group_by(col('country')) \\\n",
    "         .select(col('country'), col('id').count, col('tel').cast(DataTypes.BIGINT()).max) \\\n",
    "         .execute().print()\n",
    "    # +----+--------------------------------+----------------------+----------------------+\n",
    "    # | op |                        country |               EXPR$0 |               EXPR$1 |\n",
    "    # +----+--------------------------------+----------------------+----------------------+\n",
    "    # | +I |                        Germany |                    1 |                  123 |\n",
    "    # | +I |                            USA |                    1 |                  124 |\n",
    "    # | +I |                          China |                    1 |                  135 |\n",
    "    # | -U |                          China |                    1 |                  135 |\n",
    "    # | +U |                          China |                    2 |                  135 |\n",
    "    # +----+--------------------------------+----------------------+----------------------+\n",
    "\n",
    "    # distinct\n",
    "    table.select(col('country')).distinct() \\\n",
    "         .execute().print()\n",
    "    # +----+--------------------------------+\n",
    "    # | op |                        country |\n",
    "    # +----+--------------------------------+\n",
    "    # | +I |                        Germany |\n",
    "    # | +I |                          China |\n",
    "    # | +I |                            USA |\n",
    "    # +----+--------------------------------+\n",
    "\n",
    "    # join\n",
    "    # Note that it still doesn't support duplicate column names between the joined tables\n",
    "    table.join(right_table.rename_columns(col('id').alias('r_id')), col('id') == col('r_id')) \\\n",
    "         .execute().print()\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+----------------------+----------------------+\n",
    "    # | op |                   id |                           name |                            tel |                        country |                 r_id |                  age |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+----------------------+----------------------+\n",
    "    # | +I |                    4 |                        PyFlink |                             32 |                          China |                    4 |                   10 |\n",
    "    # | +I |                    1 |                          Flink |                            123 |                        Germany |                    1 |                   18 |\n",
    "    # | +I |                    2 |                          hello |                            135 |                          China |                    2 |                   30 |\n",
    "    # | +I |                    3 |                          world |                            124 |                            USA |                    3 |                   25 |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+----------------------+----------------------+\n",
    "\n",
    "    # join lateral\n",
    "    @udtf(result_types=[DataTypes.STRING()])\n",
    "    def split(r: Row):\n",
    "        for s in r.name.split(\"i\"):\n",
    "            yield s\n",
    "\n",
    "    table.join_lateral(split.alias('a')) \\\n",
    "         .execute().print()\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | op |                   id |                           name |                            tel |                        country |                              a |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | +I |                    1 |                          Flink |                            123 |                        Germany |                             Fl |\n",
    "    # | +I |                    1 |                          Flink |                            123 |                        Germany |                             nk |\n",
    "    # | +I |                    2 |                          hello |                            135 |                          China |                          hello |\n",
    "    # | +I |                    3 |                          world |                            124 |                            USA |                          world |\n",
    "    # | +I |                    4 |                        PyFlink |                             32 |                          China |                           PyFl |\n",
    "    # | +I |                    4 |                        PyFlink |                             32 |                          China |                             nk |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "\n",
    "    # show schema\n",
    "    table.print_schema()\n",
    "    # (\n",
    "    #   `id` BIGINT,\n",
    "    #   `name` STRING,\n",
    "    #   `tel` STRING,\n",
    "    #   `country` STRING\n",
    "    # )\n",
    "\n",
    "    # show execute plan\n",
    "    print(table.join_lateral(split.alias('a')).explain())\n",
    "    # == Abstract Syntax Tree ==\n",
    "    # LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{}])\n",
    "    # :- LogicalProject(id=[$0], name=[JSON_VALUE($1, _UTF-16LE'$.name', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR))], tel=[JSON_VALUE($1, _UTF-16LE'$.tel', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR))], country=[JSON_VALUE($1, _UTF-16LE'$.addr.country', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR))])\n",
    "    # :  +- LogicalTableScan(table=[[default_catalog, default_database, Unregistered_TableSource_249535355, source: [PythonInputFormatTableSource(id, data)]]])\n",
    "    # +- LogicalTableFunctionScan(invocation=[*org.apache.flink.table.functions.python.PythonTableFunction$1f0568d1f39bef59b4c969a5d620ba46*($0, $1, $2, $3)], rowType=[RecordType(VARCHAR(2147483647) a)], elementType=[class [Ljava.lang.Object;])\n",
    "    #\n",
    "    # == Optimized Physical Plan ==\n",
    "    # PythonCorrelate(invocation=[*org.apache.flink.table.functions.python.PythonTableFunction$1f0568d1f39bef59b4c969a5d620ba46*($0, $1, $2, $3)], correlate=[table(split(id,name,tel,country))], select=[id,name,tel,country,a], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) name, VARCHAR(2147483647) tel, VARCHAR(2147483647) country, VARCHAR(2147483647) a)], joinType=[INNER])\n",
    "    # +- Calc(select=[id, JSON_VALUE(data, _UTF-16LE'$.name', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR)) AS name, JSON_VALUE(data, _UTF-16LE'$.tel', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR)) AS tel, JSON_VALUE(data, _UTF-16LE'$.addr.country', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR)) AS country])\n",
    "    #    +- LegacyTableSourceScan(table=[[default_catalog, default_database, Unregistered_TableSource_249535355, source: [PythonInputFormatTableSource(id, data)]]], fields=[id, data])\n",
    "    #\n",
    "    # == Optimized Execution Plan ==\n",
    "    # PythonCorrelate(invocation=[*org.apache.flink.table.functions.python.PythonTableFunction$1f0568d1f39bef59b4c969a5d620ba46*($0, $1, $2, $3)], correlate=[table(split(id,name,tel,country))], select=[id,name,tel,country,a], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) name, VARCHAR(2147483647) tel, VARCHAR(2147483647) country, VARCHAR(2147483647) a)], joinType=[INNER])\n",
    "    # +- Calc(select=[id, JSON_VALUE(data, '$.name', NULL, ON EMPTY, NULL, ON ERROR) AS name, JSON_VALUE(data, '$.tel', NULL, ON EMPTY, NULL, ON ERROR) AS tel, JSON_VALUE(data, '$.addr.country', NULL, ON EMPTY, NULL, ON ERROR) AS country])\n",
    "    #    +- LegacyTableSourceScan(table=[[default_catalog, default_database, Unregistered_TableSource_249535355, source: [PythonInputFormatTableSource(id, data)]]], fields=[id, data])\n",
    "\n",
    "    # show execute plan with advice\n",
    "    print(table.join_lateral(split.alias('a')).explain(ExplainDetail.PLAN_ADVICE))\n",
    "    # == Abstract Syntax Tree ==\n",
    "    # LogicalCorrelate(correlation=[$cor2], joinType=[inner], requiredColumns=[{}])\n",
    "    # :- LogicalProject(id=[$0], name=[JSON_VALUE($1, _UTF-16LE'$.name', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR))], tel=[JSON_VALUE($1, _UTF-16LE'$.tel', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR))], country=[JSON_VALUE($1, _UTF-16LE'$.addr.country', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR))])\n",
    "    # :  +- LogicalTableScan(table=[[*anonymous_python-input-format$1*]])\n",
    "    # +- LogicalTableFunctionScan(invocation=[*org.apache.flink.table.functions.python.PythonTableFunction$720258394f6a31d31376164d23142f53*($0, $1, $2, $3)], rowType=[RecordType(VARCHAR(2147483647) a)])\n",
    "    #\n",
    "    # == Optimized Physical Plan With Advice ==\n",
    "    # PythonCorrelate(invocation=[*org.apache.flink.table.functions.python.PythonTableFunction$720258394f6a31d31376164d23142f53*($0, $1, $2, $3)], correlate=[table(*org.apache.flink.table.functions.python.PythonTableFunction$720258394f6a31d31376164d23142f53*(id,name,tel,country))], select=[id,name,tel,country,a], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) name, VARCHAR(2147483647) tel, VARCHAR(2147483647) country, VARCHAR(2147483647) a)], joinType=[INNER])\n",
    "    # +- Calc(select=[id, JSON_VALUE(data, _UTF-16LE'$.name', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR)) AS name, JSON_VALUE(data, _UTF-16LE'$.tel', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR)) AS tel, JSON_VALUE(data, _UTF-16LE'$.addr.country', FLAG(NULL), FLAG(ON EMPTY), FLAG(NULL), FLAG(ON ERROR)) AS country])\n",
    "    #    +- TableSourceScan(table=[[*anonymous_python-input-format$1*]], fields=[id, data])\n",
    "    #\n",
    "    # No available advice...\n",
    "    #\n",
    "    # == Optimized Execution Plan ==\n",
    "    # PythonCorrelate(invocation=[*org.apache.flink.table.functions.python.PythonTableFunction$720258394f6a31d31376164d23142f53*($0, $1, $2, $3)], correlate=[table(*org.apache.flink.table.functions.python.PythonTableFunction$720258394f6a31d31376164d23142f53*(id,name,tel,country))], select=[id,name,tel,country,a], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) name, VARCHAR(2147483647) tel, VARCHAR(2147483647) country, VARCHAR(2147483647) a)], joinType=[INNER])\n",
    "    # +- Calc(select=[id, JSON_VALUE(data, '$.name', NULL, ON EMPTY, NULL, ON ERROR) AS name, JSON_VALUE(data, '$.tel', NULL, ON EMPTY, NULL, ON ERROR) AS tel, JSON_VALUE(data, '$.addr.country', NULL, ON EMPTY, NULL, ON ERROR) AS country])\n",
    "    #    +- TableSourceScan(table=[[*anonymous_python-input-format$1*]], fields=[id, data])\n",
    "\n",
    "def sql_operations():\n",
    "    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())\n",
    "\n",
    "    # define the source\n",
    "    table = t_env.from_elements(\n",
    "        elements=[\n",
    "            (1, '{\"name\": \"Flink\", \"tel\": 123, \"addr\": {\"country\": \"Germany\", \"city\": \"Berlin\"}}'),\n",
    "            (2, '{\"name\": \"hello\", \"tel\": 135, \"addr\": {\"country\": \"China\", \"city\": \"Shanghai\"}}'),\n",
    "            (3, '{\"name\": \"world\", \"tel\": 124, \"addr\": {\"country\": \"USA\", \"city\": \"NewYork\"}}'),\n",
    "            (4, '{\"name\": \"PyFlink\", \"tel\": 32, \"addr\": {\"country\": \"China\", \"city\": \"Hangzhou\"}}')\n",
    "        ],\n",
    "        schema=['id', 'data'])\n",
    "\n",
    "    t_env.sql_query(\"SELECT * FROM %s\" % table) \\\n",
    "         .execute().print()\n",
    "    # +----+----------------------+--------------------------------+\n",
    "    # | op |                   id |                           data |\n",
    "    # +----+----------------------+--------------------------------+\n",
    "    # | +I |                    1 | {\"name\": \"Flink\", \"tel\": 12... |\n",
    "    # | +I |                    2 | {\"name\": \"hello\", \"tel\": 13... |\n",
    "    # | +I |                    3 | {\"name\": \"world\", \"tel\": 12... |\n",
    "    # | +I |                    4 | {\"name\": \"PyFlink\", \"tel\": ... |\n",
    "    # +----+----------------------+--------------------------------+\n",
    "\n",
    "    # execute sql statement\n",
    "    @udtf(result_types=[DataTypes.STRING(), DataTypes.INT(), DataTypes.STRING()])\n",
    "    def parse_data(data: str):\n",
    "        json_data = json.loads(data)\n",
    "        yield json_data['name'], json_data['tel'], json_data['addr']['country']\n",
    "\n",
    "    t_env.create_temporary_function('parse_data', parse_data)\n",
    "    t_env.execute_sql(\n",
    "        \"\"\"\n",
    "        SELECT *\n",
    "        FROM %s, LATERAL TABLE(parse_data(`data`)) t(name, tel, country)\n",
    "        \"\"\" % table\n",
    "    ).print()\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+-------------+--------------------------------+\n",
    "    # | op |                   id |                           data |                           name |         tel |                        country |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+-------------+--------------------------------+\n",
    "    # | +I |                    1 | {\"name\": \"Flink\", \"tel\": 12... |                          Flink |         123 |                        Germany |\n",
    "    # | +I |                    2 | {\"name\": \"hello\", \"tel\": 13... |                          hello |         135 |                          China |\n",
    "    # | +I |                    3 | {\"name\": \"world\", \"tel\": 12... |                          world |         124 |                            USA |\n",
    "    # | +I |                    4 | {\"name\": \"PyFlink\", \"tel\": ... |                        PyFlink |          32 |                          China |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+-------------+--------------------------------+\n",
    "\n",
    "    # explain sql plan\n",
    "    print(t_env.explain_sql(\n",
    "        \"\"\"\n",
    "        SELECT *\n",
    "        FROM %s, LATERAL TABLE(parse_data(`data`)) t(name, tel, country)\n",
    "        \"\"\" % table\n",
    "    ))\n",
    "    # == Abstract Syntax Tree ==\n",
    "    # LogicalProject(id=[$0], data=[$1], name=[$2], tel=[$3], country=[$4])\n",
    "    # +- LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{1}])\n",
    "    #    :- LogicalTableScan(table=[[default_catalog, default_database, Unregistered_TableSource_734856049, source: [PythonInputFormatTableSource(id, data)]]])\n",
    "    #    +- LogicalTableFunctionScan(invocation=[parse_data($cor1.data)], rowType=[RecordType:peek_no_expand(VARCHAR(2147483647) f0, INTEGER f1, VARCHAR(2147483647) f2)])\n",
    "    #\n",
    "    # == Optimized Physical Plan ==\n",
    "    # PythonCorrelate(invocation=[parse_data($1)], correlate=[table(parse_data(data))], select=[id,data,f0,f1,f2], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) data, VARCHAR(2147483647) f0, INTEGER f1, VARCHAR(2147483647) f2)], joinType=[INNER])\n",
    "    # +- LegacyTableSourceScan(table=[[default_catalog, default_database, Unregistered_TableSource_734856049, source: [PythonInputFormatTableSource(id, data)]]], fields=[id, data])\n",
    "    #\n",
    "    # == Optimized Execution Plan ==\n",
    "    # PythonCorrelate(invocation=[parse_data($1)], correlate=[table(parse_data(data))], select=[id,data,f0,f1,f2], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) data, VARCHAR(2147483647) f0, INTEGER f1, VARCHAR(2147483647) f2)], joinType=[INNER])\n",
    "    # +- LegacyTableSourceScan(table=[[default_catalog, default_database, Unregistered_TableSource_734856049, source: [PythonInputFormatTableSource(id, data)]]], fields=[id, data])\n",
    "\n",
    "    # explain sql plan with advice\n",
    "    print(t_env.explain_sql(\n",
    "        \"\"\"\n",
    "        SELECT *\n",
    "        FROM %s, LATERAL TABLE(parse_data(`data`)) t(name, tel, country)\n",
    "        \"\"\" % table, ExplainDetail.PLAN_ADVICE\n",
    "    ))\n",
    "    # == Abstract Syntax Tree ==\n",
    "    # LogicalProject(id=[$0], data=[$1], name=[$2], tel=[$3], country=[$4])\n",
    "    # +- LogicalCorrelate(correlation=[$cor1], joinType=[inner], requiredColumns=[{1}])\n",
    "    #    :- LogicalTableScan(table=[[*anonymous_python-input-format$10*]])\n",
    "    #    +- LogicalTableFunctionScan(invocation=[parse_data($cor2.data)], rowType=[RecordType:peek_no_expand(VARCHAR(2147483647) f0, INTEGER f1, VARCHAR(2147483647) f2)])\n",
    "    #\n",
    "    # == Optimized Physical Plan With Advice ==\n",
    "    # PythonCorrelate(invocation=[parse_data($1)], correlate=[table(parse_data(data))], select=[id,data,f0,f1,f2], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) data, VARCHAR(2147483647) f0, INTEGER f1, VARCHAR(2147483647) f2)], joinType=[INNER])\n",
    "    # +- TableSourceScan(table=[[*anonymous_python-input-format$10*]], fields=[id, data])\n",
    "    #\n",
    "    # No available advice...\n",
    "    #\n",
    "    # == Optimized Execution Plan ==\n",
    "    # PythonCorrelate(invocation=[parse_data($1)], correlate=[table(parse_data(data))], select=[id,data,f0,f1,f2], rowType=[RecordType(BIGINT id, VARCHAR(2147483647) data, VARCHAR(2147483647) f0, INTEGER f1, VARCHAR(2147483647) f2)], joinType=[INNER])\n",
    "    # +- TableSourceScan(table=[[*anonymous_python-input-format$10*]], fields=[id, data])\n",
    "\n",
    "def column_operations():\n",
    "    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())\n",
    "\n",
    "    # define the source\n",
    "    table = t_env.from_elements(\n",
    "        elements=[\n",
    "            (1, '{\"name\": \"Flink\", \"tel\": 123, \"addr\": {\"country\": \"Germany\", \"city\": \"Berlin\"}}'),\n",
    "            (2, '{\"name\": \"hello\", \"tel\": 135, \"addr\": {\"country\": \"China\", \"city\": \"Shanghai\"}}'),\n",
    "            (3, '{\"name\": \"world\", \"tel\": 124, \"addr\": {\"country\": \"USA\", \"city\": \"NewYork\"}}'),\n",
    "            (4, '{\"name\": \"PyFlink\", \"tel\": 32, \"addr\": {\"country\": \"China\", \"city\": \"Hangzhou\"}}')\n",
    "        ],\n",
    "        schema=['id', 'data'])\n",
    "\n",
    "    # add columns\n",
    "    table = table.add_columns(\n",
    "        col('data').json_value('$.name', DataTypes.STRING()).alias('name'),\n",
    "        col('data').json_value('$.tel', DataTypes.STRING()).alias('tel'),\n",
    "        col('data').json_value('$.addr.country', DataTypes.STRING()).alias('country'))\n",
    "\n",
    "    table.execute().print()\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | op |                   id |                           data |                           name |                            tel |                        country |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | +I |                    1 | {\"name\": \"Flink\", \"tel\": 12... |                          Flink |                            123 |                        Germany |\n",
    "    # | +I |                    2 | {\"name\": \"hello\", \"tel\": 13... |                          hello |                            135 |                          China |\n",
    "    # | +I |                    3 | {\"name\": \"world\", \"tel\": 12... |                          world |                            124 |                            USA |\n",
    "    # | +I |                    4 | {\"name\": \"PyFlink\", \"tel\": ... |                        PyFlink |                             32 |                          China |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "\n",
    "    # drop columns\n",
    "    table = table.drop_columns(col('data'))\n",
    "    table.execute().print()\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | op |                   id |                           name |                            tel |                        country |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | +I |                    1 |                          Flink |                            123 |                        Germany |\n",
    "    # | +I |                    2 |                          hello |                            135 |                          China |\n",
    "    # | +I |                    3 |                          world |                            124 |                            USA |\n",
    "    # | +I |                    4 |                        PyFlink |                             32 |                          China |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "\n",
    "    # rename columns\n",
    "    table = table.rename_columns(col('tel').alias('telephone'))\n",
    "    table.execute().print()\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | op |                   id |                           name |                      telephone |                        country |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | +I |                    1 |                          Flink |                            123 |                        Germany |\n",
    "    # | +I |                    2 |                          hello |                            135 |                          China |\n",
    "    # | +I |                    3 |                          world |                            124 |                            USA |\n",
    "    # | +I |                    4 |                        PyFlink |                             32 |                          China |\n",
    "    # +----+----------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "\n",
    "    # replace columns\n",
    "    table = table.add_or_replace_columns(\n",
    "        concat(col('id').cast(DataTypes.STRING()), '_', col('name')).alias('id'))\n",
    "    table.execute().print()\n",
    "    # +----+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | op |                             id |                           name |                      telephone |                        country |\n",
    "    # +----+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "    # | +I |                        1_Flink |                          Flink |                            123 |                        Germany |\n",
    "    # | +I |                        2_hello |                          hello |                            135 |                          China |\n",
    "    # | +I |                        3_world |                          world |                            124 |                            USA |\n",
    "    # | +I |                      4_PyFlink |                        PyFlink |                             32 |                          China |\n",
    "    # +----+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
    "\n",
    "\n",
    "def row_operations():\n",
    "    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())\n",
    "\n",
    "    # define the source\n",
    "    table = t_env.from_elements(\n",
    "        elements=[\n",
    "            (1, '{\"name\": \"Flink\", \"tel\": 123, \"addr\": {\"country\": \"Germany\", \"city\": \"Berlin\"}}'),\n",
    "            (2, '{\"name\": \"hello\", \"tel\": 135, \"addr\": {\"country\": \"China\", \"city\": \"Shanghai\"}}'),\n",
    "            (3, '{\"name\": \"world\", \"tel\": 124, \"addr\": {\"country\": \"China\", \"city\": \"NewYork\"}}'),\n",
    "            (4, '{\"name\": \"PyFlink\", \"tel\": 32, \"addr\": {\"country\": \"China\", \"city\": \"Hangzhou\"}}')\n",
    "        ],\n",
    "        schema=['id', 'data'])\n",
    "\n",
    "    # map operation\n",
    "    @udf(result_type=DataTypes.ROW([DataTypes.FIELD(\"id\", DataTypes.BIGINT()),\n",
    "                                    DataTypes.FIELD(\"country\", DataTypes.STRING())]))\n",
    "    def extract_country(input_row: Row):\n",
    "        data = json.loads(input_row.data)\n",
    "        return Row(input_row.id, data['addr']['country'])\n",
    "\n",
    "    table.map(extract_country) \\\n",
    "         .execute().print()\n",
    "    # +----+----------------------+--------------------------------+\n",
    "    # | op |                   id |                        country |\n",
    "    # +----+----------------------+--------------------------------+\n",
    "    # | +I |                    1 |                        Germany |\n",
    "    # | +I |                    2 |                          China |\n",
    "    # | +I |                    3 |                          China |\n",
    "    # | +I |                    4 |                          China |\n",
    "    # +----+----------------------+--------------------------------+\n",
    "\n",
    "    # flat_map operation\n",
    "    @udtf(result_types=[DataTypes.BIGINT(), DataTypes.STRING()])\n",
    "    def extract_city(input_row: Row):\n",
    "        data = json.loads(input_row.data)\n",
    "        yield input_row.id, data['addr']['city']\n",
    "\n",
    "    table.flat_map(extract_city) \\\n",
    "         .execute().print()\n",
    "    # +----+----------------------+--------------------------------+\n",
    "    # | op |                   f0 |                             f1 |\n",
    "    # +----+----------------------+--------------------------------+\n",
    "    # | +I |                    1 |                         Berlin |\n",
    "    # | +I |                    2 |                       Shanghai |\n",
    "    # | +I |                    3 |                        NewYork |\n",
    "    # | +I |                    4 |                       Hangzhou |\n",
    "    # +----+----------------------+--------------------------------+\n",
    "\n",
    "    # aggregate operation\n",
    "    class CountAndSumAggregateFunction(AggregateFunction):\n",
    "\n",
    "        def get_value(self, accumulator):\n",
    "            return Row(accumulator[0], accumulator[1])\n",
    "\n",
    "        def create_accumulator(self):\n",
    "            return Row(0, 0)\n",
    "\n",
    "        def accumulate(self, accumulator, input_row):\n",
    "            accumulator[0] += 1\n",
    "            accumulator[1] += int(input_row.tel)\n",
    "\n",
    "        def retract(self, accumulator, input_row):\n",
    "            accumulator[0] -= 1\n",
    "            accumulator[1] -= int(input_row.tel)\n",
    "\n",
    "        def merge(self, accumulator, accumulators):\n",
    "            for other_acc in accumulators:\n",
    "                accumulator[0] += other_acc[0]\n",
    "                accumulator[1] += other_acc[1]\n",
    "\n",
    "        def get_accumulator_type(self):\n",
    "            return DataTypes.ROW(\n",
    "                [DataTypes.FIELD(\"cnt\", DataTypes.BIGINT()),\n",
    "                 DataTypes.FIELD(\"sum\", DataTypes.BIGINT())])\n",
    "\n",
    "        def get_result_type(self):\n",
    "            return DataTypes.ROW(\n",
    "                [DataTypes.FIELD(\"cnt\", DataTypes.BIGINT()),\n",
    "                 DataTypes.FIELD(\"sum\", DataTypes.BIGINT())])\n",
    "\n",
    "    count_sum = udaf(CountAndSumAggregateFunction())\n",
    "    table.add_columns(\n",
    "            col('data').json_value('$.name', DataTypes.STRING()).alias('name'),\n",
    "            col('data').json_value('$.tel', DataTypes.STRING()).alias('tel'),\n",
    "            col('data').json_value('$.addr.country', DataTypes.STRING()).alias('country')) \\\n",
    "         .group_by(col('country')) \\\n",
    "         .aggregate(count_sum.alias(\"cnt\", \"sum\")) \\\n",
    "         .select(col('country'), col('cnt'), col('sum')) \\\n",
    "         .execute().print()\n",
    "    # +----+--------------------------------+----------------------+----------------------+\n",
    "    # | op |                        country |                  cnt |                  sum |\n",
    "    # +----+--------------------------------+----------------------+----------------------+\n",
    "    # | +I |                          China |                    3 |                  291 |\n",
    "    # | +I |                        Germany |                    1 |                  123 |\n",
    "    # +----+--------------------------------+----------------------+----------------------+\n",
    "\n",
    "    # flat_aggregate operation\n",
    "    class Top2(TableAggregateFunction):\n",
    "\n",
    "        def emit_value(self, accumulator):\n",
    "            for v in accumulator:\n",
    "                if v:\n",
    "                    yield Row(v)\n",
    "\n",
    "        def create_accumulator(self):\n",
    "            return [None, None]\n",
    "\n",
    "        def accumulate(self, accumulator, input_row):\n",
    "            tel = int(input_row.tel)\n",
    "            if accumulator[0] is None or tel > accumulator[0]:\n",
    "                accumulator[1] = accumulator[0]\n",
    "                accumulator[0] = tel\n",
    "            elif accumulator[1] is None or tel > accumulator[1]:\n",
    "                accumulator[1] = tel\n",
    "\n",
    "        def get_accumulator_type(self):\n",
    "            return DataTypes.ARRAY(DataTypes.BIGINT())\n",
    "\n",
    "        def get_result_type(self):\n",
    "            return DataTypes.ROW(\n",
    "                [DataTypes.FIELD(\"tel\", DataTypes.BIGINT())])\n",
    "\n",
    "    top2 = udtaf(Top2())\n",
    "    table.add_columns(\n",
    "            col('data').json_value('$.name', DataTypes.STRING()).alias('name'),\n",
    "            col('data').json_value('$.tel', DataTypes.STRING()).alias('tel'),\n",
    "            col('data').json_value('$.addr.country', DataTypes.STRING()).alias('country')) \\\n",
    "        .group_by(col('country')) \\\n",
    "        .flat_aggregate(top2) \\\n",
    "        .select(col('country'), col('tel')) \\\n",
    "        .execute().print()\n",
    "    # +----+--------------------------------+----------------------+\n",
    "    # | op |                        country |                  tel |\n",
    "    # +----+--------------------------------+----------------------+\n",
    "    # | +I |                          China |                  135 |\n",
    "    # | +I |                          China |                  124 |\n",
    "    # | +I |                        Germany |                  123 |\n",
    "    # +----+--------------------------------+----------------------+\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "    basic_operations()\n",
    "    sql_operations()\n",
    "    column_operations()\n",
    "    row_operations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flink-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
